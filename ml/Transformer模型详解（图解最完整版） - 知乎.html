<!doctype html>
<html lang="zh" data-hairline="true" class="itcauecng" data-theme="light"><head><meta charSet="utf-8"/><title data-rh="true">Transformer模型详解（图解最完整版） - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-rh="true" name="keywords" content="Transformer,自然语言处理,深度学习（Deep Learning）"/><meta data-rh="true" name="description" content="建议大家看一下李宏毅老师讲解的Transformer，非常简单易懂（个人觉得史上最强transformer讲解）：https://www.youtube.com/watch?v=ugWDIIOHtPA&amp;amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;amp;index=60前言Tr…"/><meta data-rh="true" property="og:title" content="Transformer模型详解（图解最完整版）"/><meta data-rh="true" property="og:url" content="https://zhuanlan.zhihu.com/p/338817680"/><meta data-rh="true" property="og:description" content="建议大家看一下李宏毅老师讲解的Transformer，非常简单易懂（个人觉得史上最强transformer讲解）：https://www.youtube.com/watch?v=ugWDIIOHtPA&amp;amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;amp;index=60前言Tr…"/><meta data-rh="true" property="og:image" content="https://pic1.zhimg.com/v2-7be8fe269991a236f000168291481c8b_720w.jpg?source=172ae18b"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="og:site_name" content="知乎专栏"/><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.81060cab.png"/><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.81060cab.png" sizes="152x152"/><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.d5793cac.png" sizes="120x120"/><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7abf3393.png" sizes="76x76"/><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.362a8eac.png" sizes="60x60"/><link crossorigin="" rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/heifetz/favicon.ico"/><link crossorigin="" rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/heifetz/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pica.zhimg.com"/><link rel="dns-prefetch" href="//picx.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><link rel="dns-prefetch" href="//static.zhihu.com"/><style data-emotion-css="9mvwt1">:root{--zhc-padding-horizontal:20px;--zhc-padding-vertical:16px;--zhc-notification-top:75px;--app-padding:16px;--app-header-height:52px;--app-max-width:640px;--app-width:1000px;--app-font-size:15px;}</style><script nonce="f59dd6da-d9ff-4315-bc84-9187480a15cc" data-web-reporter-config="{&quot;platform&quot;:&quot;web&quot;,&quot;project&quot;:&quot;heifetz&quot;}">!function(e,t){"object"==typeof exports&&"undefined"!=typeof module?t(exports):"function"==typeof define&&define.amd?define(["exports"],t):t((e=e||self).webReporter={})}(this,function(e){"use strict";var t={},n=!1,o=function(){var e,o,r,a,i;return n||(e=document.querySelector("script[data-web-reporter-config]"),o=e&&e.dataset.webReporterConfig||"{}",r=JSON.parse(o),a=r.platform,i=r.project,t={platform:a,project:i},n=!0),t};function r(e){return a(function(){return localStorage.getItem(e)})()}function a(e){return function(){try{return e.apply(void 0,arguments)}catch(e){}}}var i=a(function(e,t){var n={platform:"web",project:o().project,clientTimestamp:+new Date};!function(e,t,n){"1"===r("weber:logenabled")&&console.log("[web-reporter]%o",{type:e,base:t,data:n})}(e,n,t),function(e,t){var n=btoa(JSON.stringify(t));if("undefined"!=typeof Blob&&window.navigator&&window.navigator.sendBeacon){var o=new Blob([n],{type:"text/plain"});navigator.sendBeacon(e,o)}else{var r=new XMLHttpRequest;r.open("POST",e),r.withCredentials=!1,r.setRequestHeader("Content-Type","text/plain;charset=UTF-8"),r.send(n)}}(r("weber:api")||"https://apm.zhihu.com/collector/web_json",{type:e,base:n,data:t})});e.report=i,Object.defineProperty(e,"__esModule",{value:!0})});
</script><link href="https://static.zhihu.com/heifetz/2779.216a26f4.dad83ea78e0da2c903ba.css" crossorigin="" rel="stylesheet"/><link href="https://static.zhihu.com/heifetz/column.216a26f4.47da3f06f6dd9a2ba095.css" crossorigin="" rel="stylesheet"/><script nonce="f59dd6da-d9ff-4315-bc84-9187480a15cc">!function(){"use strict";!function(e,n){var r=[];function t(e){return function(){r.push([e,arguments])}}n.Raven={captureException:t("captureException"),captureMessage:t("captureMessage"),captureBreadcrumb:t("captureBreadcrumb")};var a,o,c,i,s,u="undefined"!=typeof DOMError;function d(e){var n=e instanceof Error||e instanceof ErrorEvent||u&&e instanceof DOMError||e instanceof DOMException;Raven.captureException(n?e:new Error(e.message||e.reason))}n.addEventListener("unhandledrejection",d),n.addEventListener("error",d,!0),a=e.src,o=e,c=function(){r.forEach(function(e){var n;(n=Raven)[e[0]].apply(n,e[1])}),n.removeEventListener("unhandledrejection",d),n.removeEventListener("error",d,!0)},i=document.head||document.getElementsByTagName("head")[0],(s=document.createElement("script")).crossOrigin=o.crossOrigin,s.dataset.sentryConfig=o["data-sentry-config"],s.onload=c,s.src=a,i.appendChild(s)}({"defer":true,"crossOrigin":"anonymous","src":"https://unpkg.zhimg.com/@cfe/sentry-script@1.3.1/dist/init.js","data-sentry-config":"{\"dsn\":\"https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224\",\"sampleRate\":0.1,\"release\":\"2055-6a2838fc\",\"ignoreErrorNames\":[\"NetworkError\",\"SecurityError\"],\"ignoreErrorsPreset\":\"ReactApp\",\"tags\":{\"app_name\":\"heifetz\"}}"},window)}();
</script></head><body class="WhiteBg-body PostIndex-body Body--isAppleDevice"><div id="root"><div class="App"><style data-emotion-css="55n9hh">.css-55n9hh{position:fixed;top:0;right:0;left:0;z-index:101;display:none;height:2px;pointer-events:none;background:#1772F6;-webkit-transform:translateX(-100%);-ms-transform:translateX(-100%);transform:translateX(-100%);}</style><div class="LoadingBar  css-55n9hh"></div><div><span style="position:absolute;top:-10000px;left:-10000px" role="log" aria-live="assertive"></span></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;初识CV&quot;,&quot;itemId&quot;:338817680,&quot;title&quot;:&quot;Transformer模型详解（图解最完整版）&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><style data-emotion-css="1l12z7y">.css-1l12z7y{box-shadow:0px 16px 32px rgba(0,0,0,0.04);}</style><div class="Sticky ColumnPageHeader css-1l12z7y"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><style data-emotion-css="1hlrcxk">.css-1hlrcxk{-webkit-transition-property:fill;transition-property:fill;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}</style><svg viewBox="0 0 64 30" fill="#1772F6" width="64" height="30" class="css-1hlrcxk"><path d="M29.05 4.582H16.733V25.94h3.018l.403 2.572 4.081-2.572h4.815V4.582zm-5.207 18.69l-2.396 1.509-.235-1.508h-1.724V7.233h6.78v16.04h-2.425zM14.46 14.191H9.982c0-.471.033-.954.039-1.458v-5.5h5.106V5.935a1.352 1.352 0 0 0-.404-.957 1.378 1.378 0 0 0-.968-.396H5.783c.028-.088.056-.177.084-.255.274-.82 1.153-3.326 1.153-3.326a4.262 4.262 0 0 0-2.413.698c-.57.4-.912.682-1.371 1.946-.532 1.453-.997 2.856-1.31 3.693C1.444 8.674.28 11.025.28 11.025a5.85 5.85 0 0 0 2.52-.61c1.119-.593 1.679-1.502 2.054-2.883l.09-.3h2.334v5.5c0 .5-.045.982-.073 1.46h-4.12c-.71 0-1.39.278-1.893.775a2.638 2.638 0 0 0-.783 1.874h6.527a17.717 17.717 0 0 1-.778 3.649 16.796 16.796 0 0 1-3.012 5.273A33.104 33.104 0 0 1 0 28.74s3.13 1.175 5.425-.954c1.388-1.292 2.631-3.814 3.23-5.727a28.09 28.09 0 0 0 1.12-5.229h5.967v-1.37a1.254 1.254 0 0 0-.373-.899 1.279 1.279 0 0 0-.909-.37z"></path><path d="M11.27 19.675l-2.312 1.491 5.038 7.458a6.905 6.905 0 0 0 .672-2.218 3.15 3.15 0 0 0-.28-2.168l-3.118-4.563zM51.449 15.195V5.842c4.181-.205 7.988-.405 9.438-.483l.851-.05c.387-.399.885-2.395.689-3.021-.073-.25-.213-.666-.638-.555a33.279 33.279 0 0 1-4.277.727c-2.766.321-3.97.404-7.804.682-6.718.487-12.709.72-12.709.72a2.518 2.518 0 0 0 .788 1.834 2.567 2.567 0 0 0 1.883.706c2.278-.095 5.598-.25 8.996-.41v9.203h-12.78c0 .703.281 1.377.783 1.874a2.69 2.69 0 0 0 1.892.777h10.105v7.075c0 .887-.464 1.192-1.231 1.214h-3.92a4.15 4.15 0 0 0 .837 1.544 4.2 4.2 0 0 0 1.403 1.067 6.215 6.215 0 0 0 2.71.277c1.36-.066 2.967-.826 2.967-3.57v-7.607h11.28c.342 0 .67-.135.91-.374.242-.239.378-.563.378-.902v-1.375H51.449z"></path><path d="M42.614 8.873a2.304 2.304 0 0 0-1.508-.926 2.334 2.334 0 0 0-1.727.405l-.376.272 4.255 5.85 2.24-1.62-2.884-3.98zM57.35 8.68l-3.125 4.097 2.24 1.663 4.517-5.927-.375-.277a2.32 2.32 0 0 0-1.722-.452 2.327 2.327 0 0 0-1.536.896z"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="//www.zhihu.com/column/c_1186688096946528256">初识CV</a></div></div><div class="ColumnPageHeader-Button"><div class="Popover"><button title="更多" id="null-toggle" aria-haspopup="true" aria-expanded="false" type="button" class="Button ColumnPageHeader-MenuToggler FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--Dots" fill="currentColor"><path d="M6 10.5a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3ZM10.5 12a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0ZM16.5 12a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Z"></path></svg></button></div><button type="button" class="Button ColumnPageHeader-WriteButton FEfUrdfMIKpQDJDqkjte Button--blue JmYzaky7MEPMFcJDLNMG"><svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--EditSurround" fill="currentColor"><path d="M4.352 6.804A2.554 2.554 0 0 1 6.905 4.25h6.072a.875.875 0 0 0 0-1.75H6.905a4.304 4.304 0 0 0-4.303 4.304v10.142a4.304 4.304 0 0 0 4.303 4.304h10.143a4.304 4.304 0 0 0 4.304-4.304v-6.071a.875.875 0 0 0-1.75 0v6.071a2.554 2.554 0 0 1-2.554 2.554H6.905a2.554 2.554 0 0 1-2.553-2.554V6.804Z"></path><path d="M20.595 4.731a.875.875 0 1 0-1.237-1.237l-7.763 7.762a.875.875 0 1 0 1.238 1.238l7.762-7.763Z"></path></svg>写文章</button></div></div><div class="ColumnPageHeader-profile"><button type="button" class="Button AppHeader-profileEntry FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><style data-emotion-css="icip60">.css-icip60{border-radius:2px;}</style><style data-emotion-css="fnnb9l">.css-fnnb9l{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#ffffff;width:30px;height:30px;border-radius:2px;}</style><img class="Avatar AppHeader-profileAvatar css-fnnb9l" src="https://pic1.zhimg.com/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpeg" srcSet="https://pic1.zhimg.com/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpeg 2x" alt="点击打开undefined的主页"/></button></div></div></div></div><style data-emotion-css="78p1r9">.css-78p1r9{box-sizing:border-box;margin:0;min-width:0;margin-left:auto;margin-right:auto;max-width:690px;margin-top:0;}@media screen and (min-width:40em){.css-78p1r9{margin-top:1em;}}</style><div class="css-78p1r9"><style data-emotion-css="a1bait">.css-a1bait{position:relative;padding-bottom:68%;height:0;border-radius:inherit;}</style><style data-emotion-css="1rbq9wv">.css-1rbq9wv{box-sizing:border-box;margin:0;min-width:0;position:relative;padding-bottom:68%;height:0;border-radius:inherit;}</style><div class="css-1rbq9wv"><style data-emotion-css="1ld0bim">.css-1ld0bim{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:inherit;}</style><div class="css-1ld0bim"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0"><picture><img alt="Transformer模型详解（图解最完整版）" decoding="async" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;filter:blur(20px);background-size:cover;background-image:url(&quot;data:image/svg+xml;charset=utf8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; width=&#x27;1&#x27; height=&#x27;1&#x27;%3E%3Crect width=&#x27;1&#x27; height=&#x27;1&#x27; fill=&#x27;%23F6F6F6&#x27;/%3E%3C/svg%3E&quot;);background-position:0% 0%" width="100%" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/></picture></span></div></div></div><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">Transformer模型详解（图解最完整版）</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><div class="AuthorInfo"><meta itemProp="name" content="初识CV"/><meta itemProp="image" content="https://picx.zhimg.com/v2-16340cfaf16380019c183d160df3bb5e_l.jpg?source=172ae18b"/><meta itemProp="url" content="https://www.zhihu.com/people/AI_team-WSF"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><a href="//www.zhihu.com/people/AI_team-WSF" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User"><style data-emotion-css="uodor8">.css-uodor8{border-radius:50%;}</style><style data-emotion-css="1syywx2">.css-1syywx2{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#ffffff;width:38px;height:38px;border-radius:50%;}</style><img class="Avatar AuthorInfo-avatar css-1syywx2" src="https://picx.zhimg.com/v2-16340cfaf16380019c183d160df3bb5e_l.jpg?source=172ae18b" srcSet="https://picx.zhimg.com/v2-16340cfaf16380019c183d160df3bb5e_l.jpg?source=172ae18b 2x" alt="初识CV"/></a></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><a href="//www.zhihu.com/people/AI_team-WSF" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User">初识CV</a><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="4gq0sj">.css-4gq0sj{box-sizing:border-box;margin:0;min-width:0;color:#09408e;display:inline-block;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-4gq0sj" aria-label="西安电子科技大学 电子科学与技术硕士" data-tooltip="西安电子科技大学 电子科学与技术硕士"><style data-emotion-css="1ldqog6">.css-1ldqog6{-webkit-transform:scale(1.1);-ms-transform:scale(1.1);transform:scale(1.1);}</style><span style="display:inline-flex;align-items:center">​<svg width="18" height="18" fill="none" viewBox="0 0 24 24" class="ZDI ZDI--BadgeCertNewStrokeLightFill24 css-1ldqog6"><path fill="#fff" d="M12 2c-2.02.05-1.86 1.2-3.38 1.83-1.48.61-2.33-.26-3.69 1.1-1.39 1.47-.46 2.17-1.1 3.69C3.2 10.14 2.05 9.98 2 12c.05 2.02 1.2 1.86 1.83 3.38.63 1.52-.3 2.22 1.1 3.69 1.36 1.35 2.21.49 3.69 1.1 1.52.63 1.36 1.78 3.38 1.83 2.02-.05 1.86-1.2 3.38-1.83 1.48-.61 2.33.26 3.69-1.1 1.39-1.47.46-2.16 1.1-3.69.63-1.52 1.78-1.36 1.83-3.38-.05-2.02-1.2-1.86-1.83-3.38-.63-1.52.3-2.22-1.1-3.69-1.36-1.35-2.21-.49-3.69-1.1C13.86 3.2 14.02 2.05 12 2Z" style="fill:#fff;fill-opacity:1"></path><path fill="url(#id-106203557-a)" fill-rule="evenodd" d="M3 12c0 1.449 1.035 1.251 1.755 2.997s-.144 2.34.882 3.366c1.026 1.026 1.62.153 3.366.882C10.749 19.965 10.551 21 12 21c1.449 0 1.251-1.035 2.997-1.755s2.34.144 3.366-.882c1.026-1.026.153-1.62.882-3.366C19.965 13.251 21 13.44 21 12c0-1.449-1.035-1.251-1.755-2.997s.144-2.34-.882-3.366c-1.026-1.026-1.62-.153-3.366-.882C13.251 4.035 13.449 3 12 3c-1.449 0-1.251 1.035-2.997 1.755s-2.34-.144-3.366.882c-1.026 1.026-.153 1.62-.882 3.366C4.035 10.749 3 10.551 3 12Z" clip-rule="evenodd"></path><path fill="#fff" fill-rule="evenodd" d="m10.003 15.355-2.37-2.699s-.413-.634.217-1.226c.63-.591 1.195-.197 1.195-.197l1.758 1.92 4.226-4.439s.597-.313 1.17.178c.572.49.243 1.244.243 1.244s-3.12 3.472-4.839 5.204c-.881.837-1.6.015-1.6.015Z" clip-rule="evenodd" style="fill:#fff;fill-opacity:1"></path><defs><linearGradient id="id-106203557-a" x1="5.7" x2="15.6" y1="5.7" y2="19.2" gradientUnits="userSpaceOnUse"><stop stop-color="#47D3FF" style="stop-color:#47d3ff;stop-opacity:1"></stop><stop offset="1" stop-color="#4A7DFF" style="stop-color:stop-opacity:1"></stop></linearGradient></defs></svg></span></a><style data-emotion-css="2dtzk2">.css-2dtzk2{cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-left:.3em;}</style><a href="https://www.zhihu.com/kvip/purchase" rel="noopener noreferrer" class="css-2dtzk2">​<style data-emotion-css="1m3x3v9">.css-1m3x3v9{width:1em;height:1em;}</style><img src="https://pic1.zhimg.com/v2-57fe7feb4813331d5eca02ef731e12c9.jpg?source=88ceefae" alt="知乎知识会员" class="css-1m3x3v9"/></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText css-0">西安电子科技大学 电子科学与技术硕士</div></div></div></div></div></div><button type="button" class="Button FollowButton FEfUrdfMIKpQDJDqkjte Button--primary Button--blue epMJl0lFQuYbC7jrwr_o JmYzaky7MEPMFcJDLNMG"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Plus FollowButton-icon" fill="currentColor"><path fill-rule="evenodd" d="M13.25 3.25a1.25 1.25 0 1 0-2.5 0v7.5h-7.5a1.25 1.25 0 1 0 0 2.5h7.5v7.5a1.25 1.25 0 1 0 2.5 0v-7.5h7.5a1.25 1.25 0 0 0 0-2.5h-7.5v-7.5Z" clip-rule="evenodd"></path></svg></span>关注他</button></div><style data-emotion-css="z4ujak">.css-z4ujak{color:#8491a5;}</style><span class="css-z4ujak"></span></header><div class="Post-RichTextContainer"><style data-emotion-css="1od93p9">.css-1od93p9{margin-top:16px;position:relative;}</style><div class="css-1od93p9"><style data-emotion-css="376mun">.css-376mun{position:relative;display:inline;}</style><div class="css-376mun"><style data-emotion-css="1dlndns">.css-1dlndns{position:absolute;left:NaNpx;top:0;}</style><style data-emotion-css="ldd79s">.css-ldd79s{box-sizing:border-box;margin:0;min-width:0;position:absolute;left:NaNpx;top:0;}</style><div class="css-ldd79s"></div><style data-emotion-css="dg64xe">.css-dg64xe .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(248,248,250,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-dg64xe .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-dg64xe .FileLinkCard-info{margin-left:12px;}.css-dg64xe .FileLinkCard-name{color:#191B1F;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-dg64xe .FileLinkCard-meta{color:#9196a1;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-dg64xe .FileLinkCard-source{white-space:pre;}.css-dg64xe img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}.css-dg64xe img.content_image[data-size="normal"],.css-dg64xe img.origin_image[data-size="normal"]{width:100%;max-width:100%;}.css-dg64xe img.content_image[data-size="small"],.css-dg64xe img.origin_image[data-size="small"]{width:320px;max-width:100%;}</style><style data-emotion-css="1vqsdx1">.css-1vqsdx1 .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#f8f8fa;}.css-1vqsdx1 .LinkCard.new,.css-1vqsdx1 .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1vqsdx1 .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-1vqsdx1 .LinkCard.new .LinkCard-contents .loading{height:14px;background:#ebeced;border-radius:7px;}.css-1vqsdx1 .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-1vqsdx1 .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#191B1F;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1vqsdx1 .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1vqsdx1 .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-1vqsdx1 .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-1vqsdx1 .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-1vqsdx1 .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#9196a1;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1vqsdx1 .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#373a40;}.css-1vqsdx1 .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1vqsdx1 .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-1vqsdx1 .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(196,199,206,0.3);}.css-1vqsdx1 .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-1vqsdx1 .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-1vqsdx1 .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#ebeced;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-1vqsdx1 .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#ebeced;color:#c4c7ce;}.css-1vqsdx1 .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#9196a1;}.css-1vqsdx1 .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-1vqsdx1 .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-1vqsdx1 .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#373a40;}.css-1vqsdx1 .LinkCard.new .LinkCard-richText .text{color:#373a40;}.css-1vqsdx1 .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-1vqsdx1 .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-1vqsdx1 .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-1vqsdx1 .LinkCard.old,.css-1vqsdx1 .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1vqsdx1 .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(248,248,250,0.88);color:#c4c7ce;}.css-1vqsdx1 .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#ebeced;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-1vqsdx1 .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-1vqsdx1 .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1vqsdx1 .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#ebeced;}.css-1vqsdx1 .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-1vqsdx1 .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}</style><style data-emotion-css="yxro2d">.css-yxro2d .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-yxro2d .LinkCard.old,.css-yxro2d .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-yxro2d .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(248,248,250,0.88);color:#c4c7ce;}.css-yxro2d .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#ebeced;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-yxro2d .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-yxro2d .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-yxro2d .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#ebeced;}.css-yxro2d .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-yxro2d .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}.css-yxro2d .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#f8f8fa;}.css-yxro2d .LinkCard.new,.css-yxro2d .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-yxro2d .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-yxro2d .LinkCard.new .LinkCard-contents .loading{height:14px;background:#ebeced;border-radius:7px;}.css-yxro2d .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-yxro2d .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#191B1F;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-yxro2d .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-yxro2d .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-yxro2d .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-yxro2d .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-yxro2d .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#9196a1;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-yxro2d .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#373a40;}.css-yxro2d .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-yxro2d .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-yxro2d .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(196,199,206,0.3);}.css-yxro2d .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-yxro2d .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-yxro2d .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#ebeced;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-yxro2d .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#ebeced;color:#c4c7ce;}.css-yxro2d .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#9196a1;}.css-yxro2d .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-yxro2d .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-yxro2d .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#373a40;}.css-yxro2d .LinkCard.new .LinkCard-richText .text{color:#373a40;}.css-yxro2d .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-yxro2d .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-yxro2d .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(248,248,250,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-yxro2d .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-yxro2d .FileLinkCard-info{margin-left:12px;}.css-yxro2d .FileLinkCard-name{color:#191B1F;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-yxro2d .FileLinkCard-meta{color:#9196a1;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-yxro2d .FileLinkCard-source{white-space:pre;}.css-yxro2d img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}.css-yxro2d img.content_image[data-size="normal"],.css-yxro2d img.origin_image[data-size="normal"]{width:100%;max-width:100%;}.css-yxro2d img.content_image[data-size="small"],.css-yxro2d img.origin_image[data-size="small"]{width:320px;max-width:100%;}</style><style data-emotion-css="ob6uua animation-1yvu044">.css-ob6uua{word-break:break-word;line-height:1.6;}.css-ob6uua > [data-first-child]{margin-top:0;}.css-ob6uua > :last-child{margin-bottom:0;}.css-ob6uua h1,.css-ob6uua h2{clear:left;margin-top:calc((1.4em * 2) / 1.2);margin-bottom:calc(1.4em / 1.2);font-size:1.2em;line-height:1.5;font-weight:600;}.css-ob6uua h3,.css-ob6uua h4,.css-ob6uua h5,.css-ob6uua h6{clear:left;margin-top:calc((1.4em * 1.5) / 1.1);margin-bottom:calc(1.4em / 1.1);font-size:1.1em;line-height:1.5;font-weight:600;}.css-ob6uua u{-webkit-text-decoration:none;text-decoration:none;border-bottom:1px solid #373a40;}.css-ob6uua b{font-weight:600;}.css-ob6uua sup{font-size:0.8em;}.css-ob6uua sup[data-draft-type='reference']{color:#09408e;}.css-ob6uua a:focus{outline:none;-webkit-transition:box-shadow 0.3s;transition:box-shadow 0.3s;}html[data-focus-visible] .css-ob6uua a:focus{box-shadow:0 0 0 2px #ffffff,0 0 0 4px rgba(23,114,246,0.3);}.css-ob6uua a.ztext-link,.css-ob6uua a.internal,.css-ob6uua a.external{-webkit-text-decoration:none;text-decoration:none;cursor:pointer;border-bottom:1px solid #81858f;}.css-ob6uua a.ztext-link:hover,.css-ob6uua a.internal:hover,.css-ob6uua a.external:hover{color:#09408e;border-bottom:1px solid #09408e;}.css-ob6uua a.ztext-link > .ellipsis::after,.css-ob6uua a.internal > .ellipsis::after,.css-ob6uua a.external > .ellipsis::after{content:'...';}.css-ob6uua a.ztext-link > .invisible,.css-ob6uua a.internal > .invisible,.css-ob6uua a.external > .invisible{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;}.css-ob6uua a.ztext-link u,.css-ob6uua a.internal u,.css-ob6uua a.external u{border:none;}.css-ob6uua a.member_mention{color:#09408e;}.css-ob6uua a.member_mention:hover{border-bottom:1px solid #09408e;}.css-ob6uua a.UserLink-link{color:#09408e;}.css-ob6uua a.UserLink-link:hover{border-bottom:1px solid #09408e;}.css-ob6uua p{margin:1.4em 0;}.css-ob6uua p.ztext-empty-paragraph{margin:calc((2.8em- (1.4em * 2 + 1.6em)) / 2) 0;}.css-ob6uua p.ztext-empty-paragraph + .ztext-empty-paragraph{margin:1.4em 0;}.css-ob6uua hr{margin:4em auto;width:240px;max-width:100%;border:none;border-top:1px solid #c4c7ce;}.css-ob6uua img[eeimg]{max-width:100%;vertical-align:middle;}.css-ob6uua img[eeimg="1"]{margin:0 3px;max-width:calc(100% - 6px);display:inline-block;}.css-ob6uua img[eeimg="2"]{margin:1.4em auto;display:block;}.css-ob6uua blockquote{margin:1.4em 0;padding-left:1em;color:#535861;border-left:3px solid #c4c7ce;}.css-ob6uua ol,.css-ob6uua ul{margin:1.4em 0;padding:0;width:100%;}.css-ob6uua ol ol,.css-ob6uua ul ol,.css-ob6uua ol ul,.css-ob6uua ul ul{margin:0;}.css-ob6uua ol li::before,.css-ob6uua ul li::before{width:1em;}.css-ob6uua ol > ol,.css-ob6uua ul > ol,.css-ob6uua ol > ul,.css-ob6uua ul > ul{padding-left:1em;box-sizing:border-box;}.css-ob6uua ul>li{display:table;width:100%;list-style:none;}.css-ob6uua ul>li::before{display:table-cell;content:'•  ';white-space:pre;}.css-ob6uua ol{counter-reset:ol;}.css-ob6uua ol > li{display:table;width:100%;list-style:none;}.css-ob6uua ol > li::before{display:table-cell;text-align:right;counter-increment:ol;content:counter(ol) '. ';white-space:pre;}.css-ob6uua ol ol{counter-reset:ol2;}.css-ob6uua ol ol li::before{counter-increment:ol2;content:counter(ol2) '. ';}.css-ob6uua ol ol ol{counter-reset:ol3;}.css-ob6uua ol ol ol li::before{counter-increment:ol3;content:counter(ol3) '. ';}.css-ob6uua ol ol ol ol{counter-reset:ol4;}.css-ob6uua ol ol ol ol li::before{counter-increment:ol4;content:counter(ol4) '. ';}.css-ob6uua figure{margin:1.4em 0;}.css-ob6uua figure .content_image,.css-ob6uua figure .origin_image{margin:0 auto;}.css-ob6uua figure figcaption{margin-top:calc(0.6em / 0.9);padding:0 1em;font-size:0.9em;line-height:1.5;text-align:center;color:#9196a1;}.css-ob6uua figure + figure{margin-top:calc(1.4em * 1.6);}.css-ob6uua figure[data-size='small'],.css-ob6uua figure:not([data-size]) > [data-size='small']{clear:both;}.css-ob6uua figure[data-size='left'],.css-ob6uua figure:not([data-size]) > [data-size='left']{float:left;margin:0 20px 20px 0;max-width:33%;}.css-ob6uua figure[data-size='right'],.css-ob6uua figure:not([data-size]) > [data-size='right']{float:right;margin:0 0 20px 20px;max-width:33%;}.css-ob6uua figure[data-size='collapse']{margin-bottom:0;}.css-ob6uua figure[data-size='collapse'] + figure{margin-top:0;}.css-ob6uua .content_image,.css-ob6uua .origin_image{display:block;max-width:100%;height:auto;margin:1.4em auto;background-color:#fff;}.css-ob6uua .content_image[data-size='small'],.css-ob6uua .origin_image[data-size='small']{max-width:40%;}.css-ob6uua .content_image.zh-lightbox-thumb,.css-ob6uua .origin_image.zh-lightbox-thumb{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}.css-ob6uua code{margin:0 2px;padding:3px 4px;border-radius:3px;font-family:Menlo,Monaco,Consolas,'Andale Mono','lucida console','Courier New',monospace;font-size:0.9em;background-color:#f8f8fa;}.css-ob6uua pre{margin:1.4em 0;padding:calc(0.8em / 0.9);font-size:0.9em;word-break:initial;word-wrap:initial;white-space:pre;overflow:auto;-webkit-overflow-scrolling:touch;background:#f8f8fa;border-radius:4px;}.css-ob6uua pre code{margin:0;padding:0;font-size:inherit;border-radius:0;background-color:inherit;}.css-ob6uua li pre{white-space:pre-wrap;}.css-ob6uua table[data-draft-type='table']{border-collapse:collapse;font-size:15px;margin:1.4em auto;max-width:100%;table-layout:fixed;text-align:left;width:100%;}.css-ob6uua table[data-draft-type='table'][data-size='small']{min-width:260px;width:40%;}.css-ob6uua table[data-draft-type='table'][data-row-style='striped'] tr:nth-of-type(2n + 1){background:#f8f8fa;}.css-ob6uua table[data-draft-type='table'] td,.css-ob6uua table[data-draft-type='table'] th{border:1px solid #c4c7ce;line-height:24px;height:24px;padding:3px 12px;}.css-ob6uua table[data-draft-type='table'] th{background:#ebeced;color:#191B1F;font-weight:500;}.css-ob6uua .video-box,.css-ob6uua .link-box{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;margin:1.4em 0;overflow:auto;white-space:normal;cursor:pointer;border:solid 1px #ebeced;border-radius:4px;}.css-ob6uua .lazy[data-lazy-status]{background-color:#f8f8fa;}.css-ob6uua .lazy[data-lazy-status="ok"]{background-color:#fff;-webkit-animation:animation-1yvu044 0.5s ease-in;animation:animation-1yvu044 0.5s ease-in;}.css-ob6uua .highlight{margin:1em 0;}.css-ob6uua .highlight pre{margin:0;}.css-ob6uua .highlight .hll{background-color:#f8f8fa;}.css-ob6uua .highlight .c{font-style:italic;color:#9196a1;}.css-ob6uua .highlight .err{color:#D95350;}.css-ob6uua .highlight .k{font-weight:600;}.css-ob6uua .highlight .o{font-weight:600;}.css-ob6uua .highlight .cm{font-style:italic;color:#9196a1;}.css-ob6uua .highlight .cp{font-weight:600;color:#9196a1;}.css-ob6uua .highlight .c1{font-style:italic;color:#9196a1;}.css-ob6uua .highlight .cs{font-style:italic;font-weight:600;color:#9196a1;}.css-ob6uua .highlight .gd{color:#F05159;}.css-ob6uua .highlight .ge{font-style:italic;}.css-ob6uua .highlight .gr{color:#D95350;}.css-ob6uua .highlight .gh{color:#9196a1;}.css-ob6uua .highlight .gi{color:#12b370;}.css-ob6uua .highlight .go{color:#81858f;}.css-ob6uua .highlight .gp{color:#535861;}.css-ob6uua .highlight .gs{font-weight:600;}.css-ob6uua .highlight .gu{color:#9196a1;}.css-ob6uua .highlight .gt{color:#D95350;}.css-ob6uua .highlight .kc{font-weight:600;}.css-ob6uua .highlight .kd{font-weight:600;}.css-ob6uua .highlight .kn{font-weight:600;}.css-ob6uua .highlight .kp{font-weight:600;}.css-ob6uua .highlight .kr{font-weight:600;}.css-ob6uua .highlight .kt{font-weight:600;color:#09408e;}.css-ob6uua .highlight .m{color:#1772F6;}.css-ob6uua .highlight .s{color:#D95350;}.css-ob6uua .highlight .na{color:#1772F6;}.css-ob6uua .highlight .nb{color:#1772F6;}.css-ob6uua .highlight .nc{font-weight:600;color:#09408e;}.css-ob6uua .highlight .no{color:#1772F6;}.css-ob6uua .highlight .ni{color:#6A5FF3;}.css-ob6uua .highlight .ne{font-weight:600;color:#D95350;}.css-ob6uua .highlight .nf{font-weight:600;color:#D95350;}.css-ob6uua .highlight .nn{color:#535861;}.css-ob6uua .highlight .nt{color:#09408e;}.css-ob6uua .highlight .nv{color:#1772F6;}.css-ob6uua .highlight .ow{font-weight:600;}.css-ob6uua .highlight .w{color:#adb0b7;}.css-ob6uua .highlight .mf{color:#1772F6;}.css-ob6uua .highlight .mh{color:#1772F6;}.css-ob6uua .highlight .mi{color:#1772F6;}.css-ob6uua .highlight .mo{color:#1772F6;}.css-ob6uua .highlight .sb{color:#D95350;}.css-ob6uua .highlight .sc{color:#D95350;}.css-ob6uua .highlight .sd{color:#D95350;}.css-ob6uua .highlight .s2{color:#D95350;}.css-ob6uua .highlight .se{color:#D95350;}.css-ob6uua .highlight .sh{color:#D95350;}.css-ob6uua .highlight .si{color:#D95350;}.css-ob6uua .highlight .sx{color:#D95350;}.css-ob6uua .highlight .sr{color:#A5542F;}.css-ob6uua .highlight .s1{color:#D95350;}.css-ob6uua .highlight .ss{color:#D95350;}.css-ob6uua .highlight .bp{color:#9196a1;}.css-ob6uua .highlight .vc{color:#1772F6;}.css-ob6uua .highlight .vg{color:#1772F6;}.css-ob6uua .highlight .vi{color:#1772F6;}.css-ob6uua .highlight .il{color:#1772F6;}.css-ob6uua .highlight::-webkit-scrollbar{width:6px;height:6px;}.css-ob6uua .highlight::-webkit-scrollbar-thumb:horizontal{background-color:rgba(25,27,31,0.5);border-radius:6px;}.css-ob6uua .highlight::-webkit-scrollbar-thumb:horizontal:hover{background-color:rgba(25,27,31,0.6);}.css-ob6uua .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-ob6uua .LinkCard.old,.css-ob6uua .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-ob6uua .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(248,248,250,0.88);color:#c4c7ce;}.css-ob6uua .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#ebeced;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-ob6uua .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-ob6uua .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-ob6uua .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#ebeced;}.css-ob6uua .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-ob6uua .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}.css-ob6uua .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#f8f8fa;}.css-ob6uua .LinkCard.new,.css-ob6uua .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-ob6uua .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-ob6uua .LinkCard.new .LinkCard-contents .loading{height:14px;background:#ebeced;border-radius:7px;}.css-ob6uua .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-ob6uua .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#191B1F;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-ob6uua .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-ob6uua .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-ob6uua .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-ob6uua .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-ob6uua .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#9196a1;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-ob6uua .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#373a40;}.css-ob6uua .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#9196a1;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-ob6uua .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-ob6uua .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(196,199,206,0.3);}.css-ob6uua .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-ob6uua .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-ob6uua .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#ebeced;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-ob6uua .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#ebeced;color:#c4c7ce;}.css-ob6uua .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#9196a1;}.css-ob6uua .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-ob6uua .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-ob6uua .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#373a40;}.css-ob6uua .LinkCard.new .LinkCard-richText .text{color:#373a40;}.css-ob6uua .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-ob6uua .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-ob6uua .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(248,248,250,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-ob6uua .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-ob6uua .FileLinkCard-info{margin-left:12px;}.css-ob6uua .FileLinkCard-name{color:#191B1F;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-ob6uua .FileLinkCard-meta{color:#9196a1;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-ob6uua .FileLinkCard-source{white-space:pre;}.css-ob6uua img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}.css-ob6uua img.content_image[data-size="normal"],.css-ob6uua img.origin_image[data-size="normal"]{width:100%;max-width:100%;}.css-ob6uua img.content_image[data-size="small"],.css-ob6uua img.origin_image[data-size="small"]{width:320px;max-width:100%;}@-webkit-keyframes animation-1yvu044{from{opacity:0;}to{opacity:1;}}@keyframes animation-1yvu044{from{opacity:0;}to{opacity:1;}}</style><div class="RichText ztext Post-RichText css-ob6uua" options="[object Object]"><blockquote data-first-child data-pid="AVhVsLox">建议大家看一下李宏毅老师讲解的Transformer，非常简单易懂（个人觉得史上最强<span data-search-entity="15">transformer</span>讲解）：<a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DugWDIIOHtPA%26list%3DPLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4%26index%3D60" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://www.</span><span class="visible">youtube.com/watch?</span><span class="invisible">v=ugWDIIOHtPA&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=60</span><span class="ellipsis"></span></a></blockquote><h2 id="h_338817680_0" data-into-catalog-status="">前言</h2><p data-pid="zDFs7RYJ">Transformer由论文<span data-search-entity="14">《Attention is All You Need》</span>提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从<span data-search-entity="4">GitHub</span>获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。</p><p data-pid="Wk6qKjMA">在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。</p><p data-pid="g_zVukPF">Attention is All You Need：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762" class=" wrap external" target="_blank" rel="nofollow noreferrer">Attention Is All You Need</a></p><h2 id="h_338817680_1" data-into-catalog-status="">1.Transformer 整体结构</h2><p data-pid="ZjxO6FU_">首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-4544255f3f24b7af1e520684ae38403f_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="438" data-original-token="v2-4544255f3f24b7af1e520684ae38403f" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic4.zhimg.com/v2-4544255f3f24b7af1e520684ae38403f_r.jpg"/><figcaption>Transformer 的整体结构，左图Encoder和右图Decoder</figcaption></figure><p data-pid="3lcsm2js">可以看到 <b>Transformer 由 Encoder 和 Decoder 两个部分组成</b>，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：</p><p data-pid="YKxxdv7K"><b>第一步：</b>获取输入句子的每一个单词的表示向量 <b>X</b>，<b>X</b>由单词的 <span data-search-entity="0">Embedding</span>（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-7dd39c44b0ae45d31a3ae7f39d3f883f_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="263" data-original-token="v2-7dd39c44b0ae45d31a3ae7f39d3f883f" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-7dd39c44b0ae45d31a3ae7f39d3f883f_r.jpg"/><figcaption>Transformer 的输入表示</figcaption></figure><p data-pid="qOjIvOCC"><b>第二步：</b>将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 <b>x</b>) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 <b>C</b>，如下图。单词向量矩阵用 <span class="ztext-math" data-eeimg="1" data-tex="X_{n\times d}">X_{n\times d}</span> 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-45db05405cb96248aff98ee07a565baa_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="900" data-original-token="v2-45db05405cb96248aff98ee07a565baa" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-45db05405cb96248aff98ee07a565baa_r.jpg"/><figcaption>Transformer Encoder 编码句子信息</figcaption></figure><p data-pid="0mHnFgSN"><b>第三步</b>：将 Encoder 输出的编码信息矩阵 <b>C</b>传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 <b>Mask (掩盖)</b> 操作遮盖住 i+1 之后的单词。</p><figure data-size="normal"><img src="https://picx.zhimg.com/v2-5367bd47a2319397317562c0da77e455_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="370" data-original-token="v2-5367bd47a2319397317562c0da77e455" class="origin_image zh-lightbox-thumb" width="640" data-original="https://picx.zhimg.com/v2-5367bd47a2319397317562c0da77e455_r.jpg"/><figcaption>Transofrmer Decoder 预测</figcaption></figure><p data-pid="UJOdIK8F">上图 Decoder 接收了 Encoder 的<span data-search-entity="12">编码矩阵</span><b> C</b>，然后首先输入一个翻译开始符 &#34;&lt;Begin&gt;&#34;，预测第一个单词 &#34;I&#34;；然后输入翻译开始符 &#34;&lt;Begin&gt;&#34; 和单词 &#34;I&#34;，预测单词 &#34;have&#34;，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。</p><h2 id="h_338817680_2" data-into-catalog-status="">2. Transformer 的输入</h2><p data-pid="P-_Ljk3V">Transformer 中单词的输入表示 <b>x</b>由<b>单词 Embedding</b> 和<b>位置 Embedding</b> （Positional Encoding）相加得到。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-b0a11f97ab22f5d9ebc396bc50fa9c3f_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="142" data-original-token="v2-b0a11f97ab22f5d9ebc396bc50fa9c3f" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-b0a11f97ab22f5d9ebc396bc50fa9c3f_r.jpg"/><figcaption>Transformer 的输入表示</figcaption></figure><h3 id="h_338817680_3" data-into-catalog-status="">2.1 单词 Embedding</h3><p data-pid="67OOnfVl">单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。</p><h3 id="h_338817680_4" data-into-catalog-status="">2.2 位置 Embedding</h3><p data-pid="3qttpwzz">Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。<b>因为 Transformer 不采用 <span data-search-entity="2">RNN</span> 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。</b>所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。</p><p data-pid="43CgJ1x5">位置 Embedding 用 <b>PE</b>表示，<b>PE</b> 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：</p><figure data-size="normal"><img src="https://pica.zhimg.com/v2-8b442ffd03ea0f103e9acc37a1db910a_1440w.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="136" data-original-token="v2-8b442ffd03ea0f103e9acc37a1db910a" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pica.zhimg.com/v2-8b442ffd03ea0f103e9acc37a1db910a_r.jpg"/></figure><p data-pid="zgBxjiuH">其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。使用这种公式计算 PE 有以下的好处：</p><ul><li data-pid="stlz419F">使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。</li><li data-pid="Dj2BhT15">可以让模型容易地计算出相对位置，对于固定长度的间距 k，<b>PE(pos+k)</b> 可以用 <b>PE(pos)</b> 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。</li></ul><p data-pid="DAcCydQ0">将单词的词 Embedding 和位置 Embedding 相加，就可以得到单词的表示向量 <b>x</b>，<b>x </b>就是 Transformer 的输入。</p><h2 id="h_338817680_5" data-into-catalog-status="">3. Self-Attention（<span data-search-entity="1">自注意力机制</span>）</h2><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-f6380627207ff4d1e72addfafeaff0bb_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="884" data-original-token="v2-f6380627207ff4d1e72addfafeaff0bb" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-f6380627207ff4d1e72addfafeaff0bb_r.jpg"/><figcaption>Transformer Encoder 和 Decoder</figcaption></figure><p data-pid="UusPTvfg">上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为<b> Multi-Head Attention</b>，是由多个 <b>Self-Attention</b>组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 <span data-search-entity="8">Layer Normalization</span>，用于对每一层的激活值进行归一化。</p><p data-pid="S_GBVCpG">因为 <b>Self-Attention</b>是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。</p><h3 id="h_338817680_6" data-into-catalog-status="">3.1 Self-Attention 结构</h3><figure data-size="normal"><img src="https://picx.zhimg.com/v2-6444601b4c41d99e70569b0ea388c3bd_1440w.jpg" data-size="normal" data-rawwidth="406" data-rawheight="488" data-original-token="v2-6444601b4c41d99e70569b0ea388c3bd" class="content_image" width="406"/><figcaption>Self-Attention 结构</figcaption></figure><p data-pid="kPoXDPJf">上图是 Self-Attention 的结构，在计算的时候需要用到矩阵<b>Q(查询),K(键值),V(值)</b>。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而<b>Q,K,V</b>正是通过 Self-Attention 的输入进行<span data-search-entity="17">线性变换</span>得到的。</p><h3 id="h_338817680_7" data-into-catalog-status="">3.2 Q, K, V 的计算</h3><p data-pid="koWF61hY">Self-Attention 的输入用矩阵X进行表示，则可以使用<span data-search-entity="6">线性变阵</span>矩阵<b>WQ,WK,WV</b>计算得到<b>Q,K,V</b>。计算如下图所示，<b>注意 X, Q, K, V 的每一行都表示一个单词。</b></p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-4f4958704952dcf2c4b652a1cd38f32e_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="875" data-original-token="v2-4f4958704952dcf2c4b652a1cd38f32e" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-4f4958704952dcf2c4b652a1cd38f32e_r.jpg"/><figcaption>Q, K, V 的计算</figcaption></figure><h3 id="h_338817680_8" data-into-catalog-status="">3.3 Self-Attention 的输出</h3><p data-pid="DA2kBPUp">得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下：</p><figure data-size="normal"><img src="https://picx.zhimg.com/v2-9699a37b96c2b62d22b312b5e1863acd_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="171" data-original-token="v2-9699a37b96c2b62d22b312b5e1863acd" class="origin_image zh-lightbox-thumb" width="640" data-original="https://picx.zhimg.com/v2-9699a37b96c2b62d22b312b5e1863acd_r.jpg"/><figcaption>Self-Attention 的输出</figcaption></figure><p data-pid="pn9xOANa">公式中计算矩阵<b>Q</b>和<b>K</b>每一行向量的内积，为了防止内积过大，因此除以  <span class="ztext-math" data-eeimg="1" data-tex="d_{k}">d_{k}</span>  的平方根。<b>Q</b>乘以<b>K</b>的转置后，得到的<span data-search-entity="13">矩阵行列数</span>都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为<b>Q</b>乘以 <span class="ztext-math" data-eeimg="1" data-tex="K^{T}">K^{T}</span> ，1234 表示的是句子中的单词。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-9caab2c9a00f6872854fb89278f13ee1_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="228" data-original-token="v2-9caab2c9a00f6872854fb89278f13ee1" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic4.zhimg.com/v2-9caab2c9a00f6872854fb89278f13ee1_r.jpg"/><figcaption>Q乘以K的转置的计算</figcaption></figure><p data-pid="WP8aAUDY">得到<span class="ztext-math" data-eeimg="1" data-tex="QK^{T}">QK^{T}</span> 之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1.</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-96a3716cf7f112f7beabafb59e84f418_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="247" data-original-token="v2-96a3716cf7f112f7beabafb59e84f418" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic3.zhimg.com/v2-96a3716cf7f112f7beabafb59e84f418_r.jpg"/><figcaption>对矩阵的每一行进行 Softmax</figcaption></figure><p data-pid="QgcSFJKu">得到 Softmax 矩阵之后可以和<b>V</b>相乘，得到最终的输出<b>Z</b>。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-7ac99bce83713d568d04e6ecfb31463b_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="217" data-original-token="v2-7ac99bce83713d568d04e6ecfb31463b" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-7ac99bce83713d568d04e6ecfb31463b_r.jpg"/><figcaption>Self-Attention 输出</figcaption></figure><p data-pid="Kn263Qnw">上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出 <span class="ztext-math" data-eeimg="1" data-tex="Z_{1}">Z_{1}</span>  等于所有单词 i 的值 <span class="ztext-math" data-eeimg="1" data-tex="V_{i}">V_{i}</span>  根据 <span data-search-entity="3">attention 系数</span>的比例加在一起得到，如下图所示：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-27822b2292cd6c38357803093bea5d0e_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="216" data-original-token="v2-27822b2292cd6c38357803093bea5d0e" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-27822b2292cd6c38357803093bea5d0e_r.jpg"/><figcaption>Zi 的计算方法</figcaption></figure><h3 id="h_338817680_9" data-into-catalog-status="">3.4 Multi-Head Attention</h3><p data-pid="S4RuF-Xc">在上一步，我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。</p><figure data-size="normal"><img src="https://picx.zhimg.com/v2-b0ea8f5b639786f98330f70405e94a75_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="859" data-original-token="v2-b0ea8f5b639786f98330f70405e94a75" class="origin_image zh-lightbox-thumb" width="640" data-original="https://picx.zhimg.com/v2-b0ea8f5b639786f98330f70405e94a75_r.jpg"/><figcaption>Multi-Head Attention</figcaption></figure><p data-pid="WUq-PPHP">从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入<b>X</b>分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵<b>Z</b>。下图是 h=8 时候的情况，此时会得到 8 个输出矩阵<b>Z</b>。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-6bdaf739fd6b827b2087b4e151c560f4_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="831" data-original-token="v2-6bdaf739fd6b827b2087b4e151c560f4" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-6bdaf739fd6b827b2087b4e151c560f4_r.jpg"/><figcaption>多个 Self-Attention</figcaption></figure><p data-pid="tEHtDHN1">得到 8 个输出矩阵 <span class="ztext-math" data-eeimg="1" data-tex="Z_{1}">Z_{1}</span>  到 <span class="ztext-math" data-eeimg="1" data-tex="Z_{8}">Z_{8}</span> 之后，Multi-Head Attention 将它们拼接在一起 <b>(Concat)</b>，然后传入一个<b>Linear</b>层，得到 Multi-Head Attention 最终的输出<b>Z</b>。</p><figure data-size="normal"><img src="https://picx.zhimg.com/v2-35d78d9aa9150ae4babd0ea6aa68d113_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="388" data-original-token="v2-35d78d9aa9150ae4babd0ea6aa68d113" class="origin_image zh-lightbox-thumb" width="640" data-original="https://picx.zhimg.com/v2-35d78d9aa9150ae4babd0ea6aa68d113_r.jpg"/><figcaption>Multi-Head Attention 的输出</figcaption></figure><p data-pid="kzWvDrWD">可以看到 Multi-Head Attention 输出的矩阵<b>Z</b>与其输入的矩阵<b>X</b>的维度是一样的。</p><h2 id="h_338817680_10" data-into-catalog-status="">4. Encoder 结构</h2><figure data-size="normal"><img src="https://picx.zhimg.com/v2-0203e83066913b53ec6f5482be092aa1_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="884" data-original-token="v2-0203e83066913b53ec6f5482be092aa1" class="origin_image zh-lightbox-thumb" width="640" data-original="https://picx.zhimg.com/v2-0203e83066913b53ec6f5482be092aa1_r.jpg"/><figcaption>Transformer Encoder block</figcaption></figure><p data-pid="J3mjSa2T">上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 Multi-Head Attention,<b> Add &amp; Norm, Feed Forward, Add &amp; Norm </b>组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。</p><h3 id="h_338817680_11" data-into-catalog-status="">4.1 Add &amp; Norm</h3><p data-pid="1kA5s52x">Add &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-a4b35db50f882522ee52f61ddd411a5a_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="107" data-original-token="v2-a4b35db50f882522ee52f61ddd411a5a" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-a4b35db50f882522ee52f61ddd411a5a_r.jpg"/><figcaption>Add &amp;amp;amp;amp;amp;amp; Norm 公式</figcaption></figure><p data-pid="D3Zf-0rM">其中 <b>X</b>表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(<b>X</b>) 和 FeedForward(<b>X</b>) 表示输出 (输出与输入 <b>X </b>维度是一样的，所以可以相加)。</p><p data-pid="GAmriVAD"><b>Add</b>指 <b>X</b>+MultiHeadAttention(<b>X</b>)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 <span data-search-entity="5">ResNet</span> 中经常用到：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-4b3dde965124bd00f9893b05ebcaad0f_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="117" data-original-token="v2-4b3dde965124bd00f9893b05ebcaad0f" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic4.zhimg.com/v2-4b3dde965124bd00f9893b05ebcaad0f_r.jpg"/><figcaption>残差连接</figcaption></figure><p data-pid="WfJc0LoJ"><b>Norm</b>指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。</p><h3 id="h_338817680_12" data-into-catalog-status="">4.2 Feed Forward</h3><p data-pid="iOC9_HOt">Feed Forward 层比较简单，是一个两层的<span data-search-entity="7">全连接层</span>，第一层的激活函数为 Relu，第二层不使用<span data-search-entity="11">激活函数</span>，对应的公式如下。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-47b39ca4cc3cd0be157d6803c8c8e0a1_1440w.jpg" data-size="normal" data-rawwidth="596" data-rawheight="77" data-original-token="v2-47b39ca4cc3cd0be157d6803c8c8e0a1" class="origin_image zh-lightbox-thumb" width="596" data-original="https://pic4.zhimg.com/v2-47b39ca4cc3cd0be157d6803c8c8e0a1_r.jpg"/><figcaption>Feed Forward</figcaption></figure><p data-pid="UBzq-OVA"><b>X</b>是输入，Feed Forward 最终得到的输出矩阵的维度与<b>X</b>一致。</p><h3 id="h_338817680_13" data-into-catalog-status="">4.3 组成 Encoder</h3><p data-pid="H1K4fEUt">通过上面描述的 Multi-Head Attention, Feed Forward, Add &amp; Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵  <span class="ztext-math" data-eeimg="1" data-tex="X_{(n\times d)}">X_{(n\times d)}</span> ，并输出一个矩阵  <span class="ztext-math" data-eeimg="1" data-tex="O_{(n\times d)}">O_{(n\times d)}</span> 。通过多个 Encoder block 叠加就可以组成 Encoder。</p><p data-pid="2c4xDaYY">第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是<b>编码信息矩阵 C</b>，这一矩阵后续会用到 Decoder 中。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-45db05405cb96248aff98ee07a565baa_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="900" data-original-token="v2-45db05405cb96248aff98ee07a565baa" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-45db05405cb96248aff98ee07a565baa_r.jpg"/><figcaption>Encoder 编码句子信息</figcaption></figure><h2 id="h_338817680_14" data-into-catalog-status="">5. Decoder 结构</h2><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-f5049e8711c3abe8f8938ced9e7fc3da_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="884" data-original-token="v2-f5049e8711c3abe8f8938ced9e7fc3da" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic3.zhimg.com/v2-f5049e8711c3abe8f8938ced9e7fc3da_r.jpg"/><figcaption>Transformer Decoder block</figcaption></figure><p data-pid="-GJFcQbQ">上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：</p><ul><li data-pid="uQ15qepT">包含两个 Multi-Head Attention 层。</li><li data-pid="OYZubYSx">第一个 Multi-Head Attention 层采用了 Masked 操作。</li><li data-pid="1YA1p36E">第二个 Multi-Head Attention 层的<b>K, V</b>矩阵使用 Encoder 的<b>编码信息矩阵C</b>进行计算，而<b>Q</b>使用上一个 Decoder block 的输出计算。</li><li data-pid="rB_j5s55">最后有一个 Softmax 层计算下一个翻译单词的概率。</li></ul><h3 id="h_338817680_15" data-into-catalog-status="">5.1 第一个 Multi-Head Attention</h3><p data-pid="AXeguOLh">Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 &#34;我有一只猫&#34; 翻译成 &#34;I have a cat&#34; 为例，了解一下 Masked 操作。</p><p data-pid="Xsgegc-q">下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 &#34;&lt;Begin&gt;&#34; 预测出第一个单词为 &#34;I&#34;，然后根据输入 &#34;&lt;Begin&gt; I&#34; 预测下一个单词 &#34;have&#34;。</p><figure data-size="normal"><img src="https://pica.zhimg.com/v2-4616451fe8aa59b2df2ead30fa31dc98_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="140" data-original-token="v2-4616451fe8aa59b2df2ead30fa31dc98" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pica.zhimg.com/v2-4616451fe8aa59b2df2ead30fa31dc98_r.jpg"/><figcaption>Decoder 预测</figcaption></figure><p data-pid="ePRyx-DQ">Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 (&lt;Begin&gt; I have a cat) 和对应输出 (I have a cat &lt;end&gt;) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，<b>注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 &#34;&lt;Begin&gt; I have a cat &lt;end&gt;&#34;。</b></p><p data-pid="FPU6iWoo"><b>第一步：</b>是 Decoder 的输入矩阵和 <b>Mask </b>矩阵，输入矩阵包含 &#34;&lt;Begin&gt; I have a cat&#34; (0, 1, 2, 3, 4) 五个单词的表示向量，<b>Mask </b>是一个 5×5 的矩阵。在 <b>Mask </b>可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。</p><figure data-size="normal"><img src="https://pica.zhimg.com/v2-b26299d383aee0dd42b163e8bda74fc8_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="277" data-original-token="v2-b26299d383aee0dd42b163e8bda74fc8" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pica.zhimg.com/v2-b26299d383aee0dd42b163e8bda74fc8_r.jpg"/><figcaption>输入矩阵与 Mask 矩阵</figcaption></figure><p data-pid="g8lAyIkQ"><b>第二步：</b>接下来的操作和之前的 Self-Attention 一样，通过输入矩阵<b>X</b>计算得到<b>Q,K,V</b>矩阵。然后计算<b>Q</b>和 <span class="ztext-math" data-eeimg="1" data-tex="K^{T}">K^{T}</span> 的乘积 <span class="ztext-math" data-eeimg="1" data-tex="QK^{T}">QK^{T}</span> 。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-a63ff9b965595438ed0c0e0547cd3d3b_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="240" data-original-token="v2-a63ff9b965595438ed0c0e0547cd3d3b" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-a63ff9b965595438ed0c0e0547cd3d3b_r.jpg"/><figcaption>Q乘以K的转置</figcaption></figure><p data-pid="zFwaFths"><b>第三步：</b>在得到 <span class="ztext-math" data-eeimg="1" data-tex="QK^{T}">QK^{T}</span>  之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用<b>Mask</b>矩阵遮挡住每一个单词之后的信息，遮挡操作如下：</p><figure data-size="normal"><img src="https://picx.zhimg.com/v2-35d1c8eae955f6f4b6b3605f7ef00ee1_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="206" data-original-token="v2-35d1c8eae955f6f4b6b3605f7ef00ee1" class="origin_image zh-lightbox-thumb" width="640" data-original="https://picx.zhimg.com/v2-35d1c8eae955f6f4b6b3605f7ef00ee1_r.jpg"/><figcaption>Softmax 之前 Mask</figcaption></figure><p data-pid="LrBaRp1I">得到 <b>Mask</b> <span class="ztext-math" data-eeimg="1" data-tex="QK^{T}">QK^{T}</span>  之后在 <b>Mask </b><span class="ztext-math" data-eeimg="1" data-tex="QK^{T}">QK^{T}</span>上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p><p data-pid="ZKqdTv1L"><b>第四步：</b>使用 <b>Mask </b><span class="ztext-math" data-eeimg="1" data-tex="QK^{T}">QK^{T}</span>与矩阵<b> V</b>相乘，得到输出 <b>Z</b>，则单词 1 的输出向量 <span class="ztext-math" data-eeimg="1" data-tex="Z_{1}">Z_{1}</span> 是只包含单词 1 信息的。</p><figure data-size="normal"><img src="https://picx.zhimg.com/v2-58f916c806a6981e296a7a699151af87_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="292" data-original-token="v2-58f916c806a6981e296a7a699151af87" class="origin_image zh-lightbox-thumb" width="640" data-original="https://picx.zhimg.com/v2-58f916c806a6981e296a7a699151af87_r.jpg"/><figcaption>Mask 之后的输出</figcaption></figure><p data-pid="4_h9O2No"><b>第五步：</b>通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 <span class="ztext-math" data-eeimg="1" data-tex="Z_{i}">Z_{i}</span> ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出<span class="ztext-math" data-eeimg="1" data-tex="Z_{i}">Z_{i}</span> 然后计算得到第一个 Multi-Head Attention 的输出<b>Z</b>，<b>Z</b>与输入<b>X</b>维度一样。</p><h3 id="h_338817680_16" data-into-catalog-status="">5.2 第二个 Multi-Head Attention</h3><p data-pid="BFqZNiKC">Decoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 <b>K, V</b>矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 <b>Encoder 的<span data-search-entity="9">编码信息矩阵</span> C </b>计算的。</p><p data-pid="DPtM9EeZ">根据 Encoder 的输出 <b>C</b>计算得到 <b>K, V</b>，根据上一个 Decoder block 的输出<b> Z</b> 计算 <b>Q</b> (如果是第一个 Decoder block 则使用输入矩阵 <b>X</b> 进行计算)，后续的计算方法与之前描述的一致。</p><p data-pid="AcK1hMgP">这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 <b>Mask</b>)。</p><h3 id="h_338817680_17" data-into-catalog-status="">5.3 Softmax 预测输出单词</h3><p data-pid="ScHGVY2r">Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的<span data-search-entity="10">网络层</span>我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-335cfa1b345bdd5cf1e212903bb9b185_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="239" data-original-token="v2-335cfa1b345bdd5cf1e212903bb9b185" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-335cfa1b345bdd5cf1e212903bb9b185_r.jpg"/><figcaption>Decoder Softmax 之前的 Z</figcaption></figure><p data-pid="5QPkXQBP">Softmax 根据输出矩阵的每一行预测下一个单词：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-0938aa45a288b5d6bef6487efe53bd9d_1440w.jpg" data-size="normal" data-rawwidth="640" data-rawheight="357" data-original-token="v2-0938aa45a288b5d6bef6487efe53bd9d" class="origin_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-0938aa45a288b5d6bef6487efe53bd9d_r.jpg"/><figcaption>Decoder Softmax 预测</figcaption></figure><p data-pid="6jjQp-dz">这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。</p><h2 id="h_338817680_18" data-into-catalog-status="">6. Transformer 总结</h2><ul><li data-pid="hI3YKOP1">Transformer 与 RNN 不同，可以比较好地并行训练。</li><li data-pid="3PSOM3Hw">Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个<span data-search-entity="16">词袋模型</span>了。</li><li data-pid="5fAv8TFH">Transformer 的重点是 Self-Attention 结构，其中用到的 <b>Q, K, V</b>矩阵通过输出进行线性变换得到。</li><li data-pid="B77KoJQW">Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。</li></ul><p data-pid="tFcdbALf"><sup data-text="论文:Attention Is All You Need" data-url="https://arxiv.org/abs/1706.03762" data-numero="1" data-draft-node="inline" data-draft-type="reference" data-tooltip="论文:Attention Is All You Need &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/1706.03762&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_1_0" href="#ref_1" data-reference-link="true" aria-labelledby="ref_1">[1]</a></sup><sup data-text="Transformer 模型详解" data-url="https://baijiahao.baidu.com/s?id=1651219987457222196&amp;wfr=spider&amp;for=pc" data-numero="2" data-draft-node="inline" data-draft-type="reference" data-tooltip="Transformer 模型详解 &lt;a href=&quot;https://baijiahao.baidu.com/s?id=1651219987457222196&amp;wfr=spider&amp;for=pc&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://baijiahao.baidu.com/s?id=1651219987457222196&amp;wfr=spider&amp;for=pc&lt;/a&gt;" data-tooltip-richtext="1" data-tooltip-preset="white" data-tooltip-classname="ztext-reference-tooltip"><a id="ref_2_0" href="#ref_2" data-reference-link="true" aria-labelledby="ref_2">[2]</a></sup></p><h2>参考</h2><ol class="ReferenceList"><li id="ref_1" tabindex="0"><a class="ReferenceList-backLink" href="#ref_1_0" aria-label="back" data-reference-link="true">^</a><span>论文:Attention Is All You Need</span> <a href="https://arxiv.org/abs/1706.03762" class="external" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1706.03762</a></li><li id="ref_2" tabindex="0"><a class="ReferenceList-backLink" href="#ref_2_0" aria-label="back" data-reference-link="true">^</a><span>Transformer 模型详解</span> <a href="https://baijiahao.baidu.com/s?id=1651219987457222196&amp;wfr=spider&amp;for=pc" class="external" target="_blank" rel="noopener noreferrer">https://baijiahao.baidu.com/s?id=1651219987457222196&amp;wfr=spider&amp;for=pc</a></li></ol></div></div></div></div><div role="button" tabindex="0" class="ContentItem-time">编辑于 2024-05-08 15:25<!-- -->・IP 属地未知</div><div class="PostIndex-Contributions"><h3 class="BlockTitle">内容所属专栏</h3><style data-emotion-css="zein01">.css-zein01{border-radius:4px;padding:0 12px;border:1px solid #ebeced;}</style><ul class="css-zein01"><div><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="//www.zhihu.com/column/c_1186688096946528256"><style data-emotion-css="1xlfegr">.css-1xlfegr{background:transparent;box-shadow:none;}</style><style data-emotion-css="1gomreu">.css-1gomreu{position:relative;display:inline-block;}</style><div class="css-1gomreu"><style data-emotion-css="1u7r5c9">.css-1u7r5c9{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#ffffff;width:40px;height:40px;border-radius:50%;}</style><img class="Avatar css-1u7r5c9" src="https://pic1.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_l.jpg?source=172ae18b" srcSet="https://pic1.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_l.jpg?source=172ae18b 2x" alt="初识CV"/></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><span><a class="ColumnLink ColumnItem-Title" href="//www.zhihu.com/column/c_1186688096946528256"><div class="css-1gomreu">初识CV</div></a></span></h2><div class="ContentItem-meta">从这里开始认识人类的眼睛——计算机视觉</div></div><div class="ContentItem-extra"><button type="button" class="Button FollowButton FEfUrdfMIKpQDJDqkjte Button--primary Button--blue epMJl0lFQuYbC7jrwr_o JmYzaky7MEPMFcJDLNMG">订阅专栏</button></div></div></div></div><div><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="//www.zhihu.com/column/c_1173652984163610624"><div class="css-1gomreu"><img class="Avatar css-1u7r5c9" src="https://picx.zhimg.com/v2-674781ef13a310d6045598d915896623_l.jpg?source=172ae18b" srcSet="https://picx.zhimg.com/v2-674781ef13a310d6045598d915896623_l.jpg?source=172ae18b 2x" alt="南湖研究院"/></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><span><a class="ColumnLink ColumnItem-Title" href="//www.zhihu.com/column/c_1173652984163610624"><div class="css-1gomreu">南湖研究院</div></a></span></h2><div class="ContentItem-meta">数据竞赛经验，工作积累的笔记，南湖边有可爱的橘猫</div></div><div class="ContentItem-extra"><button type="button" class="Button FollowButton FEfUrdfMIKpQDJDqkjte Button--primary Button--blue epMJl0lFQuYbC7jrwr_o JmYzaky7MEPMFcJDLNMG">订阅专栏</button></div></div></div></div><div><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="//www.zhihu.com/column/c_1339338855846608896"><div class="css-1gomreu"><img class="Avatar css-1u7r5c9" src="https://pica.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_l.jpg?source=172ae18b" srcSet="https://pica.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_l.jpg?source=172ae18b 2x" alt="深度视觉与自然语言探究"/></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><span><a class="ColumnLink ColumnItem-Title" href="//www.zhihu.com/column/c_1339338855846608896"><div class="css-1gomreu">深度视觉与自然语言探究</div></a></span></h2><div class="ContentItem-meta">记录不断发展的计算机知识。</div></div><div class="ContentItem-extra"><button type="button" class="Button FollowButton FEfUrdfMIKpQDJDqkjte Button--primary Button--blue epMJl0lFQuYbC7jrwr_o JmYzaky7MEPMFcJDLNMG">订阅专栏</button></div></div></div></div></ul></div><div class="Reward"><div><div class="Reward-tagline">真诚赞赏，手留余香</div><button class="Reward-rewardBtn">赞赏</button></div><div class="Reward-countZero">还没有人赞赏，快来当第一个赞赏的人吧！</div></div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><style data-emotion-css="ch8ocw">.css-ch8ocw{position:relative;display:inline-block;height:30px;padding:0 12px;font-size:14px;line-height:30px;color:#1772F6;vertical-align:top;border-radius:100px;background:rgba(23,114,246,0.1);}.css-ch8ocw:hover{background-color:rgba(23,114,246,0.15);}</style><div class="Tag Topic css-ch8ocw"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20746363" target="_blank"><div class="css-1gomreu">Transformer</div></a></span></div><div class="Tag Topic css-ch8ocw"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19560026" target="_blank"><div class="css-1gomreu">自然语言处理</div></a></span></div><div class="Tag Topic css-ch8ocw"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19813032" target="_blank"><div class="css-1gomreu">深度学习（Deep Learning）</div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 1.2 万" aria-live="polite" type="button" class="Button VoteButton VoteButton--up FEfUrdfMIKpQDJDqkjte"><span style="display:inline-flex;align-items:center">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor"><path fill-rule="evenodd" d="M13.792 3.681c-.781-1.406-2.803-1.406-3.584 0l-7.79 14.023c-.76 1.367.228 3.046 1.791 3.046h15.582c1.563 0 2.55-1.68 1.791-3.046l-7.79-14.023Z" clip-rule="evenodd"></path></svg></span>赞同 1.2 万</button><button aria-label="反对" aria-live="polite" type="button" class="Button VoteButton VoteButton--down FEfUrdfMIKpQDJDqkjte"><span style="display:inline-flex;align-items:center">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleDown" fill="currentColor"><path fill-rule="evenodd" d="M13.792 20.319c-.781 1.406-2.803 1.406-3.584 0L2.418 6.296c-.76-1.367.228-3.046 1.791-3.046h15.582c1.563 0 2.55 1.68 1.791 3.046l-7.79 14.023Z" clip-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Comment Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>545 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Share Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M19.47 1.914a.8.8 0 0 1 1.204.778l-1.872 16.386a.9.9 0 0 1-1.204.743l-4.615-1.692a.7.7 0 0 0-.831.28l-1.927 3.02c-.43.674-1.474.369-1.474-.43v-3.865a.8.8 0 0 1 .179-.504l5.808-7.148a.595.595 0 0 0-.897-.781l-5.93 6.354a1.1 1.1 0 0 1-1.258.252L2.57 13.46a.8.8 0 0 1-.08-1.415l16.98-10.13Z"></path></svg></span>分享</button></div></div><button aria-live="polite" type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Heart Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path fill-rule="evenodd" d="M17.142 3.041c1.785.325 3.223 1.518 4.167 3.071 1.953 3.215.782 7.21-1.427 9.858a23.968 23.968 0 0 1-4.085 3.855c-.681.5-1.349.923-1.962 1.234-.597.303-1.203.532-1.748.587a.878.878 0 0 1-.15.002c-.545-.04-1.162-.276-1.762-.582a14.845 14.845 0 0 1-2.008-1.27 24.254 24.254 0 0 1-4.21-4.002c-2.1-2.56-3.16-6.347-1.394-9.463.92-1.624 2.362-2.892 4.173-3.266 1.657-.341 3.469.097 5.264 1.44 1.75-1.309 3.516-1.76 5.142-1.464Z" clip-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Star Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M10.484 3.307c.673-1.168 2.358-1.168 3.032 0l2.377 4.122a.25.25 0 0 0 .165.12l4.655.987c1.319.28 1.84 1.882.937 2.884l-3.186 3.535a.25.25 0 0 0-.063.193l.5 4.733c.142 1.34-1.222 2.33-2.453 1.782l-4.346-1.938a.25.25 0 0 0-.204 0l-4.346 1.938c-1.231.549-2.595-.442-2.453-1.782l.5-4.733a.25.25 0 0 0-.064-.193L2.35 11.42c-.903-1.002-.382-2.604.937-2.884l4.655-.987a.25.25 0 0 0 .164-.12l2.378-4.122Z"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--withLabel fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY RuuQ6TOh2cRzJr6WlyQp"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Deliver Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><g fill-rule="evenodd" clip-rule="evenodd"><path d="M7.821 12a.75.75 0 0 1 .75-.75h6.857a.75.75 0 0 1 0 1.5H8.571a.75.75 0 0 1-.75-.75ZM8.965 8a.75.75 0 0 1 .75-.75h4.571a.75.75 0 0 1 0 1.5H9.715a.75.75 0 0 1-.75-.75Z"></path><path d="M7.527 3.15a2.35 2.35 0 0 0-2.309 1.91L3.165 15.84a.85.85 0 0 0-.015.16v2.5a2.35 2.35 0 0 0 2.35 2.35h13a2.35 2.35 0 0 0 2.35-2.35V16a.848.848 0 0 0-.015-.16L18.78 5.06a2.35 2.35 0 0 0-2.308-1.91H7.527Zm0 1.7a.65.65 0 0 0-.639.528l-1.88 9.872h13.984l-1.88-9.872a.65.65 0 0 0-.64-.528H7.528Z"></path></g></svg></span>申请转载</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false"><button type="button" class="Button FEfUrdfMIKpQDJDqkjte Button--plain Button--withIcon Button--iconOnly fEPKGkUK5jyc4fUuT0QP B46v1Ak6Gj5sL2JTS4PY hIwDV_tcL6XN1HprrnAq"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Dots Button-zi t2ntD6J1DemdOdvh5FB4" fill="currentColor"><path d="M6 10.5a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3ZM10.5 12a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0ZM16.5 12a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Z"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><style data-emotion-css="1ildg7g">.css-1ildg7g{margin-top:20px;position:relative;}</style><div class="css-1ildg7g"><style data-emotion-css="194mey8">.css-194mey8{border:1px solid;border-color:rgba(247,122,49,0.08);border-radius:6px;overflow:hidden;background-image:url(https://static.zhihu.com/heifetz/assets/zhimg_mobile_bg_case1@3x.e0a80b7f.png);background-size:343px;background-repeat:no-repeat;position:absolute;width:100%;height:100%;pointer-events:none;}</style><div class="css-194mey8"></div><style data-emotion-css="z3m2ag">.css-z3m2ag{padding:16px 14px;}.css-z3m2ag>svg{display:block;}</style><div class="css-z3m2ag"><svg width="66" height="16" viewBox="0 0 66 16" fill="none"><path fill="#36393F" fill-rule="evenodd" d="M0 4.15h2.278s.12-.426.255-1.248h.631c-.018.776-.06 2.392-.134 3.787H.342v2.246h2.49A15.79 15.79 0 0 1 .476 15.08h2.658c.739-1.06 1.837-4.025 2.116-6.146h2.145V6.689h-2.01c.055-1.353.108-3 .133-3.787h1.878V.657h-4.6c.016-.21.03-.43.043-.657H.566A20.765 20.765 0 0 1 0 4.15Zm7.737 10.93H5.478c-.104-.959-.451-2.825-.68-4.033h2.26l.138.748c.191 1.022.417 2.229.541 3.286ZM8.333.669v14.414h1.595l.146.918 1.478-.918h1.72c.429 0 .84-.167 1.143-.465.302-.298.473-.702.473-1.123V.668H8.333Zm4.512 12.227h-.635l-1.145.717-.114-.717h-.572V2.854h2.463l.003 10.04Zm4.618-10.227c1.77.016 3.714.005 5.659-.042l.091 5.449h-6.302v2.245h6.34l.039 2.34h-1.883l-.344 2.42h3.2c.354 0 .694-.138.945-.385.251-.246.392-.58.392-.93V10.32h6.192V8.075H25.6v-5.53c2.025-.084 3.971-.212 5.64-.394V0c-4.792.534-9.343.534-13.777.534v2.134Zm12.48 4.8h-2.388l.46-4.432h2.388l-.46 4.432Zm-11.206 0h2.387l-.458-4.179h-2.388l.46 4.18Z" clip-rule="evenodd"></path><path fill="url(#id-1251837015-a)" fill-rule="evenodd" d="M53.908 0h2.282l.004 12.853c-.049 1.653-.47 2.442-.47 2.442h-2.287s.471-.863.471-2.638V3.434h-2.925V1.508h2.925V0Zm8.324.802L62.12.083h-2.33l.11.72h-3.007v1.963H66V.802h-3.768ZM57.83 3.283l.106 1.057h-1.042v2.737h2.043v-.832h4.733v.832H66V4.34h-.994l.12-1.057h-2.33l-.122 1.057h-2.408l-.106-1.057h-2.33Zm4.404 3.556.09.866H66v2.039H60.45c-.016.275-.035.54-.056.796h4.148a1.191 1.191 0 0 1 .845.345 1.154 1.154 0 0 1 .337.836c-.01.296-.02.579-.03.746-.039.72-.174 2.198-.42 2.834h-3.747l.429-1.76h1.506c.04-.299.07-.599.085-.9h-3.394c-.184 1.198-.415 2.08-.652 2.66h-2.33c.505-1.208.888-3.142.966-5.557h-1.244V7.705h3.217l-.09-.866h2.213Zm-9.007-2.714c0 2.6 0 8.415-.046 9.917h-1.978m2.024-9.917Zm0 0h-1.988Zm-1.988 0c-.003 1.1 0 2.31.003 3.518Zm-16.456 6.792h2.236s.33-.968.365-3.566l1.814-.267V4.82l-1.812.266V2.932h1.812V.667h-1.812V.084H35.15v.583h-1.38v2.265h1.38v2.482l-1.391.204v2.266l1.39-.204c-.028 1.98-.365 3.237-.365 3.237Zm5.875 0h2.237c.16-.564.326-1.576.467-2.835l1.596-.178V5.64l-1.39.154c.064-.917.113-1.886.136-2.86h1.805v5.003a1.584 1.584 0 0 0 .484 1.136 1.639 1.639 0 0 0 1.16.466h1.136l.471-2.264h-.987V1.812a1.13 1.13 0 0 0-.348-.81 1.17 1.17 0 0 0-.828-.332h-2.884a41.041 41.041 0 0 0-.016-.586h-2.235l.004.586h-1.493v2.264h1.466a67.353 67.353 0 0 1-.156 3.114l-1.31.146v2.264l1.093-.121a28.027 28.027 0 0 1-.408 2.58Zm8.283-.03-3.58-.194.863 1.024c-.353.442-1.501 1.727-2.883 1.652-.867-.047-1.393-.288-2.059-.594a9.143 9.143 0 0 0-3.692-.968c-1.421-.076-2.856.306-3.872.723L33.57 15.1c.665-.384 2.302-1.15 3.892-1.064 1.357.073 2.075.402 2.834.751.76.35 1.6.735 2.918.806 2.103.114 3.703-1.258 4.495-2.12l1.016 1.206.215-3.793Z" clip-rule="evenodd"></path><defs><linearGradient id="id-1251837015-a" x1="43.836" x2="38.981" y1="2.27" y2="16.913" gradientUnits="userSpaceOnUse"><stop stop-color="#FF4E3C"></stop><stop offset="1" stop-color="#FF7223"></stop></linearGradient></defs></svg></div><style data-emotion-css="1pq6zw9">.css-1pq6zw9>div+div{margin-top:14px;}.css-1pq6zw9>div:last-child{-webkit-mask-image:linear-gradient(180deg,rgba(255,255,255,0.7) 0%,rgba(255,255,255,0.0) 100%);mask-image:linear-gradient(180deg,rgba(255,255,255,0.7) 0%,rgba(255,255,255,0.0) 100%);}</style><div class="css-1pq6zw9"><style data-emotion-css="1qi12lm">.css-1qi12lm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:0px 16px;}</style><div class="css-1qi12lm"><style data-emotion-css="1890z0z">.css-1890z0z{color:#FF501A;line-height:22px;width:22px;margin-right:4px;font-weight:500;}</style><div class="css-1890z0z">1</div><style data-emotion-css="1rr4qq7">.css-1rr4qq7{-webkit-flex:1;-ms-flex:1;flex:1;}</style><div class="css-1rr4qq7"><style data-emotion-css="1eu205s">.css-1eu205s{font-size:15px;font-weight:500;line-height:22px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}</style><div class="css-1eu205s">任天堂 Switch 2 首支预告片公开，视频中有哪些细节值得关注？</div><style data-emotion-css="xcna97">.css-xcna97{font-size:12px;font-weight:400;line-height:16px;margin-top:4px;color:#adb0b7;}</style><div class="css-xcna97">487<!-- --> 万热度</div></div></div><div class="css-1qi12lm"><div class="css-1890z0z">2</div><div class="css-1rr4qq7"><div class="css-1eu205s">上海成为中国第一个 5 万亿元GDP城市，这个数字是什么概念？</div><div class="css-xcna97">390<!-- --> 万热度</div></div></div><div class="css-1qi12lm"><div class="css-1890z0z">3</div><div class="css-1rr4qq7"><div class="css-1eu205s">蚂蚁集团确认支付宝出现重大事故，所有订单优惠 20%，到底哪里出错了？谁将为这次错误买单？</div><div class="css-xcna97">246<!-- --> 万热度</div></div></div><div class="css-1qi12lm"><style data-emotion-css="ec4p7w">.css-ec4p7w{color:#9196a1;line-height:22px;width:22px;margin-right:4px;font-weight:500;}</style><div class="css-ec4p7w">4</div><div class="css-1rr4qq7"><div class="css-1eu205s">传万科总裁祝九胜被公安机关带走，政府专班已介入，万科或面临接管改组，具体情况如何？对地产行业有何影响？</div><div class="css-xcna97">106<!-- --> 万热度</div></div></div></div><style data-emotion-css="utbeq">.css-utbeq{padding-top:12px;padding-bottom:22px;text-align:center;}</style><div class="css-utbeq"><style data-emotion-css="1w8zlnt">.css-1w8zlnt{color:#9196a1;font-size:13px;line-height:18.2px;font-weight:400;}</style><span class="css-1w8zlnt">查看更多 <span style="display:inline-flex;align-items:center">​<svg width="16" height="16" viewBox="0 0 16 16" class="ZDI ZDI--ArrowRightAlt16" fill="currentColor"><path d="M10.727 7.48a.63.63 0 0 1 0 1.039l-4.299 2.88c-.399.268-.926-.028-.926-.519V5.12c0-.491.527-.787.926-.52l4.299 2.881Z"></path></svg></span></span></div></div></div></div></main></div></div><script id="js-clientConfig" type="text/json">{"fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","lens":"https:\u002F\u002Flens.zhihu.com","zhida":"https:\u002F\u002Fzhida.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fapi\u002F","walletpay":"https:\u002F\u002Fwalletpay.zhihu.com","captcha":"https:\u002F\u002Fcaptcha.zhihu.com","vzuu":"https:\u002F\u002Fv.vzuu.com","openapi":"https:\u002F\u002Fopenapi.zhihu.com","svip":"https:\u002F\u002Fsvip.zhihu.com"},"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","videoHost":"video.zhihu.com","zhuanlanHost":"zhuanlan.zhihu.com","allowSignUp":true,"refreshValidityPeriod":"30","release":"2055-6a2838fc","currentEntry":"column","isMobileEntry":false,"apollo":{"env":"prod","globalSilence":"","ncgModeSign":"3f8e56febda4fb3bbea72e379d76de1e","logMobileUnAuth":"1","topstory_rec_adp":"1","topstory_hot_adp":"1","editor_adapt_native":"0","editor_auto_rotate":"0","enable_request_filter":"1","balanceModalSign":"ChYHAcB5ihJECkAFnhDAYmdhsTWVJoNc","reportBackendPublishError":"1","dynamic_font_schema":"1","enable_vzuu_login":"office","za_zse_ck_refer":"1","test_canary":"member|0-100,1-0","use_new_player":"member|0-100,1-0","player_vendor":"member|0-100,1-0,2-0","use_hevc":"member|0-0,1-100","upload_use_signature":"member|0-0,1-100","use_backdrop_blur":"member|0-0,1-100","article_title_imagex":"member|0-0,1-100","play_station":"member|0-0,1-100","use_cached_supported_countries":"device|1-100,0-0","contentItem_cover_imagex":"member|0-0,1-100","use_qrcode_login_v2":"device|1-100,0-0"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{},"cities":{"cityData":[]}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false,"article\u002FloadPostSearchEntity\u002F":false,"HOT_SPOT\u002FloadHotSpotList\u002F338817680":false}},"entities":{"users":{"2da331a7fd49661a3a43dbbe4a1218d4":{"uid":1010566847850311700,"userType":"people","id":"2da331a7fd49661a3a43dbbe4a1218d4"},"AI_team-WSF":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-16340cfaf16380019c183d160df3bb5e.jpg?source=172ae18b","uid":"754475696817983488","userType":"people","isFollowing":false,"urlToken":"AI_team-WSF","id":"7d0bf3ef6a7f044754895a3752969515","description":"个人研究方向：1. 图像（语音，视频）分类、图像（视频）检测、图像（视频）分割、图像（视频）降噪、图像（视频）超分；2. 模型压缩（模型量化，模型剪枝，模型蒸馏，模型稀疏）；3. 脉冲神经网络。欢迎大家一起交流学习～","name":"初识CV","isAdvertiser":false,"headline":"美好年华，扬起理想之帆，踏上新的征程加油！","gender":1,"url":"\u002Fpeople\u002F7d0bf3ef6a7f044754895a3752969515","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-16340cfaf16380019c183d160df3bb5e_l.jpg?source=172ae18b","isOrg":false,"type":"people","kvipInfo":{"isVip":true,"vipIcon":{"url":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-57fe7feb4813331d5eca02ef731e12c9.jpg?source=88ceefae","nightModeUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-57fe7feb4813331d5eca02ef731e12c9.jpg?source=88ceefae"},"targetUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fkvip\u002Fpurchase"},"badge":[{"type":"identity","topics":[],"description":"西安电子科技大学 电子科学与技术硕士"}],"badgeV2":{"title":"西安电子科技大学 电子科学与技术硕士","mergedBadges":[{"type":"identity","detailType":"identity","title":"认证","description":"西安电子科技大学 电子科学与技术硕士","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F96956163","sources":[],"icon":"","nightIcon":"","badgeStatus":"passed"}],"detailBadges":[{"type":"identity","detailType":"identity_people","title":"已认证的个人","description":"西安电子科技大学 电子科学与技术硕士","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F96956163","sources":[],"icon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","nightIcon":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","badgeStatus":"passed"}],"icon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","nightIcon":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c"},"exposedMedal":{"medalId":"972473506181627904","medalName":"有求必应","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-8fd9ecf5c5bdc63538431780023863a7_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8fd9ecf5c5bdc63538431780023863a7_l.png?source=172ae18b","description":"受邀回答 15 个问题","medalAvatarFrame":""}}},"questions":{},"answers":{},"articles":{"338817680":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fcontent_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=BiBUKF0xBSkqGGNeA2h4B1p3CminX6M1jhzi&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__&zid=__ZONEID__"],"entityWords":[{"name":"Embedding","mention":"Embedding","matchorder":1,"begin":1756,"end":1765,"entityid":-7834769299752248000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=Embedding&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0010298455758940284,"attachedInfoBytes":"sgJdCglFbWJlZGRpbmcSB1Vua25vd24Y3A0g5Q0oATXi+4Y6OgdhcnRpY2xlQPi2hvnQ2tOikwFIClIkZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2Uw8gLKAQokZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2UwGgdhcnRpY2xlIgkxNjM0MjI5Nzk6CUVtYmVkZGluZ0D4tob50NrTopMBSABSEmFpX2VudGl0eV9yZWNhbGxlcl3i+4Y6Yl0KCUVtYmVkZGluZxIHVW5rbm93bhjcDSDlDSgBNeL7hjo6B2FydGljbGVA+LaG+dDa06KTAUgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"自注意力机制","mention":"自注意力机制","matchorder":1,"begin":5838,"end":5844,"entityid":-3567487060183302000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Psychology","score":0.0009682072682414278,"attachedInfoBytes":"sgJpChLoh6rms6jmhI\u002FlipvmnLrliLYSClBzeWNob2xvZ3kYzi0g1C0oATVKz306OgdhcnRpY2xlQPv\u002F4ue4re++zgFIClIkZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2Uw8gLfAQokZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2UwGgdhcnRpY2xlIgkxNjM0MjI5Nzk6EuiHquazqOaEj+WKm+acuuWItkD7\u002F+LnuK3vvs4BSAFSEmFpX2VudGl0eV9yZWNhbGxlcl1Kz306YmkKEuiHquazqOaEj+WKm+acuuWIthIKUHN5Y2hvbG9neRjOLSDULSgBNUrPfTo6B2FydGljbGVA+\u002F\u002Fi57it777OAUgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"RNN","mention":"RNN","matchorder":1,"begin":4629,"end":4632,"entityid":2780658432212348000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=RNN&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0009523817800756174,"attachedInfoBytes":"sgJWCgNSTk4SB1Vua25vd24YlSQgmCQoATVCqXk6OgdhcnRpY2xlQOPZ2O3Iw7jLJkgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTDyArwBCiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTAaB2FydGljbGUiCTE2MzQyMjk3OToDUk5OQOPZ2O3Iw7jLJkgCUhJhaV9lbnRpdHlfcmVjYWxsZXJdQql5OmJWCgNSTk4SB1Vua25vd24YlSQgmCQoATVCqXk6OgdhcnRpY2xlQOPZ2O3Iw7jLJkgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"attention 系数","mention":"attention 系数","matchorder":3,"begin":9318,"end":9330,"entityid":-8458506982897502000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=3&q=attention+%E7%B3%BB%E6%95%B0&zhida_source=entity","docType":"","topicToken":""},"entityClass":"OtherTerm","score":0.0009169581300444984,"attachedInfoBytes":"sgJmChBhdHRlbnRpb24g57O75pWwEglPdGhlclRlcm0Y5kgg8kgoAzUFYHA6OgdhcnRpY2xlQObw7pHtltbOigFIClIkZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2Uw8gLaAQokZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2UwGgdhcnRpY2xlIgkxNjM0MjI5Nzk6EGF0dGVudGlvbiDns7vmlbBA5vDuke2W1s6KAUgDUhJhaV9lbnRpdHlfcmVjYWxsZXJdBWBwOmJmChBhdHRlbnRpb24g57O75pWwEglPdGhlclRlcm0Y5kgg8kgoAzUFYHA6OgdhcnRpY2xlQObw7pHtltbOigFIClIkZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2Uw","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"GitHub","mention":"GitHub","matchorder":1,"begin":651,"end":657,"entityid":3214201982339396600,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=GitHub&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.00228827914702201,"attachedInfoBytes":"sgJZCgZHaXRIdWISB1Vua25vd24YiwUgkQUoATX09hU7OgdhcnRpY2xlQIvoyI3G9cjNLEgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTDyAsIBCiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTAaB2FydGljbGUiCTE2MzQyMjk3OToGR2l0SHViQIvoyI3G9cjNLEgEUhJhaV9lbnRpdHlfcmVjYWxsZXJd9PYVO2JZCgZHaXRIdWISB1Vua25vd24YiwUgkQUoATX09hU7OgdhcnRpY2xlQIvoyI3G9cjNLEgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"ResNet","mention":"ResNet","matchorder":1,"begin":14435,"end":14441,"entityid":-3588552780313727500,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=ResNet&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0010083504757409933,"attachedInfoBytes":"sgJaCgZSZXNOZXQSB1Vua25vd24Y43Ag6XAoATWhKoQ6OgdhcnRpY2xlQMfao\u002F2lyLmZzgFIClIkZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2Uw8gLEAQokZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2UwGgdhcnRpY2xlIgkxNjM0MjI5Nzk6BlJlc05ldEDH2qP9pci5mc4BSAVSEmFpX2VudGl0eV9yZWNhbGxlcl2hKoQ6YloKBlJlc05ldBIHVW5rbm93bhjjcCDpcCgBNaEqhDo6B2FydGljbGVAx9qj\u002FaXIuZnOAUgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"线性变阵","mention":"线性变阵","matchorder":1,"begin":7429,"end":7433,"entityid":7990587517212428000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=%E7%BA%BF%E6%80%A7%E5%8F%98%E9%98%B5&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0009677830349976713,"attachedInfoBytes":"sgJfCgznur\u002FmgKflj5jpmLUSB1Vua25vd24YhTogiTooATXSsn06OgdhcnRpY2xlQN6R+PCsoJHybkgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTDyAs4BCiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTAaB2FydGljbGUiCTE2MzQyMjk3OToM57q\u002F5oCn5Y+Y6Zi1QN6R+PCsoJHybkgGUhJhaV9lbnRpdHlfcmVjYWxsZXJd0rJ9OmJfCgznur\u002FmgKflj5jpmLUSB1Vua25vd24YhTogiTooATXSsn06OgdhcnRpY2xlQN6R+PCsoJHybkgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"全连接层","mention":"全连接层","matchorder":1,"begin":15045,"end":15049,"entityid":-8949524726085361000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Computer","score":0.000844805900331258,"attachedInfoBytes":"sgJhCgzlhajov57mjqXlsYISCENvbXB1dGVyGMV1IMl1KAE193VdOjoHYXJ0aWNsZUDEyu2I2tW55oMBSApSJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMPIC0QEKJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMBoHYXJ0aWNsZSIJMTYzNDIyOTc5Ogzlhajov57mjqXlsYJAxMrtiNrVueaDAUgHUhJhaV9lbnRpdHlfcmVjYWxsZXJd93VdOmJhCgzlhajov57mjqXlsYISCENvbXB1dGVyGMV1IMl1KAE193VdOjoHYXJ0aWNsZUDEyu2I2tW55oMBSApSJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMA==","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"Layer Normalization","mention":"Layer Normalization","matchorder":1,"begin":6623,"end":6642,"entityid":-5667825251182457000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=Layer+Normalization&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.001995519827051928,"attachedInfoBytes":"sgJnChNMYXllciBOb3JtYWxpemF0aW9uEgdVbmtub3duGN8zIPIzKAE1RMcCOzoHYXJ0aWNsZUCBn5+y6t71q7EBSApSJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMPIC3gEKJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMBoHYXJ0aWNsZSIJMTYzNDIyOTc5OhNMYXllciBOb3JtYWxpemF0aW9uQIGfn7Lq3vWrsQFICFISYWlfZW50aXR5X3JlY2FsbGVyXUTHAjtiZwoTTGF5ZXIgTm9ybWFsaXphdGlvbhIHVW5rbm93bhjfMyDyMygBNUTHAjs6B2FydGljbGVAgZ+fsure9auxAUgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"编码信息矩阵","mention":"编码信息矩阵","matchorder":5,"begin":2353,"end":2359,"entityid":-7561720937661573000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=5&q=%E7%BC%96%E7%A0%81%E4%BF%A1%E6%81%AF%E7%9F%A9%E9%98%B5&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Computer","score":0.0007019891213262963,"attachedInfoBytes":"sgJnChLnvJbnoIHkv6Hmga\u002Fnn6npmLUSCENvbXB1dGVyGLESILcSKAU1sQU4OjoHYXJ0aWNsZUCDs7Gg79vXh5cBSApSJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMPIC3QEKJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMBoHYXJ0aWNsZSIJMTYzNDIyOTc5OhLnvJbnoIHkv6Hmga\u002Fnn6npmLVAg7OxoO\u002Fb14eXAUgJUhJhaV9lbnRpdHlfcmVjYWxsZXJdsQU4OmJnChLnvJbnoIHkv6Hmga\u002Fnn6npmLUSCENvbXB1dGVyGLESILcSKAU1sQU4OjoHYXJ0aWNsZUCDs7Gg79vXh5cBSApSJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMA==","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"网络层","mention":"网络层","matchorder":1,"begin":22479,"end":22482,"entityid":6755852831708522000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=%E7%BD%91%E7%BB%9C%E5%B1%82&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Computer","score":0.0006693149713536545,"attachedInfoBytes":"sgJfCgnnvZHnu5zlsYISCENvbXB1dGVyGM+vASDSrwEoATX4dC86OgdhcnRpY2xlQNnPtuazi+fgXUgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTDyAssBCiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTAaB2FydGljbGUiCTE2MzQyMjk3OToJ572R57uc5bGCQNnPtuazi+fgXUgKUhJhaV9lbnRpdHlfcmVjYWxsZXJd+HQvOmJfCgnnvZHnu5zlsYISCENvbXB1dGVyGM+vASDSrwEoATX4dC86OgdhcnRpY2xlQNnPtuazi+fgXUgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"激活函数","mention":"激活函数","matchorder":2,"begin":15054,"end":15058,"entityid":-542207717370342660,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=2&q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Math","score":0.0006303742188661365,"attachedInfoBytes":"sgJdCgzmv4DmtLvlh73mlbASBE1hdGgYznUg0nUoAjWzPyU6OgdhcnRpY2xlQOmlqd3Cnuy8+AFIClIkZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2Uw8gLNAQokZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2UwGgdhcnRpY2xlIgkxNjM0MjI5Nzk6DOa\u002FgOa0u+WHveaVsEDppandwp7svPgBSAtSEmFpX2VudGl0eV9yZWNhbGxlcl2zPyU6Yl0KDOa\u002FgOa0u+WHveaVsBIETWF0aBjOdSDSdSgCNbM\u002FJTo6B2FydGljbGVA6aWp3cKe7Lz4AUgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"编码矩阵","mention":"编码矩阵","matchorder":1,"begin":3617,"end":3621,"entityid":-1138836629444330900,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=%E7%BC%96%E7%A0%81%E7%9F%A9%E9%98%B5&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Computer","score":0.0005989496712981851,"attachedInfoBytes":"sgJhCgznvJbnoIHnn6npmLUSCENvbXB1dGVyGKEcIKUcKAE11QIdOjoHYXJ0aWNsZUCR9ZHm6MOCmfABSApSJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMPIC0QEKJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMBoHYXJ0aWNsZSIJMTYzNDIyOTc5OgznvJbnoIHnn6npmLVAkfWR5ujDgpnwAUgMUhJhaV9lbnRpdHlfcmVjYWxsZXJd1QIdOmJhCgznvJbnoIHnn6npmLUSCENvbXB1dGVyGKEcIKUcKAE11QIdOjoHYXJ0aWNsZUCR9ZHm6MOCmfABSApSJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMA==","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"矩阵行列数","mention":"矩阵行列数","matchorder":1,"begin":8615,"end":8620,"entityid":2994981477340221000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=%E7%9F%A9%E9%98%B5%E8%A1%8C%E5%88%97%E6%95%B0&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Math","score":0.0005641837059798399,"attachedInfoBytes":"sgJfCg\u002Fnn6npmLXooYzliJfmlbASBE1hdGgYp0MgrEMoATW65RM6OgdhcnRpY2xlQMia9M2R+ZPIKUgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTDyAtEBCiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTAaB2FydGljbGUiCTE2MzQyMjk3OToP55+p6Zi16KGM5YiX5pWwQMia9M2R+ZPIKUgNUhJhaV9lbnRpdHlfcmVjYWxsZXJduuUTOmJfCg\u002Fnn6npmLXooYzliJfmlbASBE1hdGgYp0MgrEMoATW65RM6OgdhcnRpY2xlQMia9M2R+ZPIKUgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"《Attention is All You Need》","mention":"《Attention is All You Need》","matchorder":1,"begin":583,"end":610,"entityid":-1962448439592204800,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=%E3%80%8AAttention+is+All+You+Need%E3%80%8B&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.008976412601921169,"attachedInfoBytes":"sgJzCh\u002FjgIpBdHRlbnRpb24gaXMgQWxsIFlvdSBOZWVk44CLEgdVbmtub3duGMcEIOIEKAE1zhETPDoHYXJ0aWNsZUC0i9ngqvH+4eQBSApSJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMPIC9gEKJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMBoHYXJ0aWNsZSIJMTYzNDIyOTc5Oh\u002FjgIpBdHRlbnRpb24gaXMgQWxsIFlvdSBOZWVk44CLQLSL2eCq8f7h5AFIDlISYWlfZW50aXR5X3JlY2FsbGVyXc4REzxicwof44CKQXR0ZW50aW9uIGlzIEFsbCBZb3UgTmVlZOOAixIHVW5rbm93bhjHBCDiBCgBNc4REzw6B2FydGljbGVAtIvZ4Krx\u002FuHkAUgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"transformer","mention":"transformer","matchorder":1,"begin":74,"end":85,"entityid":-5858382494348407000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=transformer&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Math","score":0.0008428525359380767,"attachedInfoBytes":"sgJaCgt0cmFuc2Zvcm1lchIETWF0aBhKIFUoATXg8lw6OgdhcnRpY2xlQKCj\u002FI+RhbbZrgFIClIkZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2Uw8gLJAQokZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2UwGgdhcnRpY2xlIgkxNjM0MjI5Nzk6C3RyYW5zZm9ybWVyQKCj\u002FI+RhbbZrgFID1ISYWlfZW50aXR5X3JlY2FsbGVyXeDyXDpiWgoLdHJhbnNmb3JtZXISBE1hdGgYSiBVKAE14PJcOjoHYXJ0aWNsZUCgo\u002FyPkYW22a4BSApSJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMA==","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"词袋模型","mention":"词袋模型","matchorder":1,"begin":23698,"end":23702,"entityid":8629681790995644000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Unknown","score":0.0007024089917187781,"attachedInfoBytes":"sgJhCgzor43ooovmqKHlnosSB1Vua25vd24YkrkBIJa5ASgBNd8hODo6B2FydGljbGVAxYHP5967suF3SApSJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMPIC3QEKJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMBoHYXJ0aWNsZSIJMTYzNDIyOTc5Ogzor43ooovmqKHlnotAxYHP5967suF3SBBSH25ld19iYWlkdS13aWtpcGVkaWEtaXRlbS1yZWNhbGxd3yE4OmJhCgzor43ooovmqKHlnosSB1Vua25vd24YkrkBIJa5ASgBNd8hODo6B2FydGljbGVAxYHP5967suF3SApSJGYyYzE0YzI5LWI4MjItNDdhZi1iMDJiLTRmYzUxZTkxNjNlMA==","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"线性变换","mention":"线性变换","matchorder":1,"begin":7338,"end":7342,"entityid":-4009093955292666000,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Math","score":0.0005714825717971911,"attachedInfoBytes":"sgJdCgznur\u002FmgKflj5jmjaISBE1hdGgYqjkgrjkoATWMzxU6OgdhcnRpY2xlQLXO9dyeybWuyAFIClIkZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2Uw8gLNAQokZjJjMTRjMjktYjgyMi00N2FmLWIwMmItNGZjNTFlOTE2M2UwGgdhcnRpY2xlIgkxNjM0MjI5Nzk6DOe6v+aAp+WPmOaNokC1zvXcnsm1rsgBSBFSEmFpX2VudGl0eV9yZWNhbGxlcl2MzxU6Yl0KDOe6v+aAp+WPmOaNohIETWF0aBiqOSCuOSgBNYzPFTo6B2FydGljbGVAtc713J7Jta7IAUgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""},{"name":"Linear层","mention":"Linear层","matchorder":1,"begin":3646,"end":3653,"entityid":1080760329416492200,"isBookMark":false,"link":{"linkType":10,"linkUrl":"https:\u002F\u002Fzhida.zhihu.com\u002Fsearch?content_id=163422979&content_type=Article&match_order=1&q=Linear%E5%B1%82&zhida_source=entity","docType":"","topicToken":""},"entityClass":"Math","score":0.0010051953777670541,"attachedInfoBytes":"sgJZCglMaW5lYXLlsYISBE1hdGgYvhwgxRwoATXDwIM6OgdhcnRpY2xlQMGI0ZGyuej\u002FDkgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTDyAsUBCiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTAaB2FydGljbGUiCTE2MzQyMjk3OToJTGluZWFy5bGCQMGI0ZGyuej\u002FDkgSUhJhaV9lbnRpdHlfcmVjYWxsZXJdw8CDOmJZCglMaW5lYXLlsYISBE1hdGgYvhwgxRwoATXDwIM6OgdhcnRpY2xlQMGI0ZGyuej\u002FDkgKUiRmMmMxNGMyOS1iODIyLTQ3YWYtYjAyYi00ZmM1MWU5MTYzZTA=","isOnAB":false,"isNatural":1,"isDelete":false,"contentType":"","contentId":"","contentToken":""}],"id":338817680,"title":"Transformer模型详解（图解最完整版）","type":"article","articleType":"normal","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F338817680","imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7be8fe269991a236f000168291481c8b_720w.jpg?source=172ae18b","titleImage":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7be8fe269991a236f000168291481c8b_720w.jpg?source=172ae18b","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4544255f3f24b7af1e520684ae38403f_200x112.png\" data-caption=\"Transformer 的整体结构，左图Encoder和右图Decoder\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"438\" data-watermark=\"original\" data-original-src=\"v2-4544255f3f24b7af1e520684ae38403f\" data-watermark-src=\"v2-8b9149193d6e7db5a0d2e8bb5c750670\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4544255f3f24b7af1e520684ae38403f_r.png\"\u002F\u003E建议大家看一下李宏毅老师讲解的Transformer，非常简单易懂（个人觉得史上最强transformer讲解）：\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.youtube.com\u002Fwatch%3Fv%3DugWDIIOHtPA%26list%3DPLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4%26index%3D60\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002Fwww.\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Eyoutube.com\u002Fwatch?\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Ev=ugWDIIOHtPA&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=60\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E前言Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub…","created":1608644015,"updated":1715153136,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-16340cfaf16380019c183d160df3bb5e.jpg?source=172ae18b","uid":"754475696817983488","userType":"people","isFollowing":false,"urlToken":"AI_team-WSF","id":"7d0bf3ef6a7f044754895a3752969515","description":"个人研究方向：1. 图像（语音，视频）分类、图像（视频）检测、图像（视频）分割、图像（视频）降噪、图像（视频）超分；2. 模型压缩（模型量化，模型剪枝，模型蒸馏，模型稀疏）；3. 脉冲神经网络。欢迎大家一起交流学习～","name":"初识CV","isAdvertiser":false,"headline":"美好年华，扬起理想之帆，踏上新的征程加油！","gender":1,"url":"\u002Fpeople\u002F7d0bf3ef6a7f044754895a3752969515","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-16340cfaf16380019c183d160df3bb5e_l.jpg?source=172ae18b","isOrg":false,"type":"people","kvipInfo":{"isVip":true,"vipIcon":{"url":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-57fe7feb4813331d5eca02ef731e12c9.jpg?source=88ceefae","nightModeUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-57fe7feb4813331d5eca02ef731e12c9.jpg?source=88ceefae"},"targetUrl":"https:\u002F\u002Fwww.zhihu.com\u002Fkvip\u002Fpurchase"},"badge":[{"type":"identity","topics":[],"description":"西安电子科技大学 电子科学与技术硕士"}],"badgeV2":{"title":"西安电子科技大学 电子科学与技术硕士","mergedBadges":[{"type":"identity","detailType":"identity","title":"认证","description":"西安电子科技大学 电子科学与技术硕士","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F96956163","sources":[],"icon":"","nightIcon":"","badgeStatus":"passed"}],"detailBadges":[{"type":"identity","detailType":"identity_people","title":"已认证的个人","description":"西安电子科技大学 电子科学与技术硕士","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F96956163","sources":[],"icon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","nightIcon":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","badgeStatus":"passed"}],"icon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c","nightIcon":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2ddc5cc683982648f6f123616fb4ec09_l.png?source=32738c0c"},"exposedMedal":{"medalId":"972473506181627904","medalName":"有求必应","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-8fd9ecf5c5bdc63538431780023863a7_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8fd9ecf5c5bdc63538431780023863a7_l.png?source=172ae18b","description":"受邀回答 15 个问题","medalAvatarFrame":""}},"commentPermission":"all","copyrightPermission":"need_review","state":"published","ipInfo":"IP 属地未知","imageWidth":640,"imageHeight":438,"content":"\u003Cblockquote data-pid=\"AVhVsLox\"\u003E建议大家看一下李宏毅老师讲解的Transformer，非常简单易懂（个人觉得史上最强transformer讲解）：\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.youtube.com\u002Fwatch%3Fv%3DugWDIIOHtPA%26list%3DPLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4%26index%3D60\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002Fwww.\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Eyoutube.com\u002Fwatch?\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Ev=ugWDIIOHtPA&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=60\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fblockquote\u003E\u003Ch2\u003E前言\u003C\u002Fh2\u003E\u003Cp data-pid=\"zDFs7RYJ\"\u003ETransformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。\u003C\u002Fp\u003E\u003Cp data-pid=\"Wk6qKjMA\"\u003E在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。\u003C\u002Fp\u003E\u003Cp data-pid=\"g_zVukPF\"\u003EAttention is All You Need：\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fabs\u002F1706.03762\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EAttention Is All You Need\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E1.Transformer 整体结构\u003C\u002Fh2\u003E\u003Cp data-pid=\"ZjxO6FU_\"\u003E首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4544255f3f24b7af1e520684ae38403f_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"438\" data-original-token=\"v2-4544255f3f24b7af1e520684ae38403f\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4544255f3f24b7af1e520684ae38403f_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ETransformer 的整体结构，左图Encoder和右图Decoder\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"3lcsm2js\"\u003E可以看到 \u003Cb\u003ETransformer 由 Encoder 和 Decoder 两个部分组成\u003C\u002Fb\u003E，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：\u003C\u002Fp\u003E\u003Cp data-pid=\"YKxxdv7K\"\u003E\u003Cb\u003E第一步：\u003C\u002Fb\u003E获取输入句子的每一个单词的表示向量 \u003Cb\u003EX\u003C\u002Fb\u003E，\u003Cb\u003EX\u003C\u002Fb\u003E由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7dd39c44b0ae45d31a3ae7f39d3f883f_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"263\" data-original-token=\"v2-7dd39c44b0ae45d31a3ae7f39d3f883f\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7dd39c44b0ae45d31a3ae7f39d3f883f_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ETransformer 的输入表示\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"qOjIvOCC\"\u003E\u003Cb\u003E第二步：\u003C\u002Fb\u003E将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 \u003Cb\u003Ex\u003C\u002Fb\u003E) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 \u003Cb\u003EC\u003C\u002Fb\u003E，如下图。单词向量矩阵用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X_%7Bn%5Ctimes+d%7D\" alt=\"X_{n\\times d}\" eeimg=\"1\"\u002F\u003E 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-45db05405cb96248aff98ee07a565baa_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"900\" data-original-token=\"v2-45db05405cb96248aff98ee07a565baa\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-45db05405cb96248aff98ee07a565baa_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ETransformer Encoder 编码句子信息\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"0mHnFgSN\"\u003E\u003Cb\u003E第三步\u003C\u002Fb\u003E：将 Encoder 输出的编码信息矩阵 \u003Cb\u003EC\u003C\u002Fb\u003E传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 \u003Cb\u003EMask (掩盖)\u003C\u002Fb\u003E 操作遮盖住 i+1 之后的单词。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5367bd47a2319397317562c0da77e455_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"370\" data-original-token=\"v2-5367bd47a2319397317562c0da77e455\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5367bd47a2319397317562c0da77e455_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ETransofrmer Decoder 预测\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"UJOdIK8F\"\u003E上图 Decoder 接收了 Encoder 的编码矩阵\u003Cb\u003E C\u003C\u002Fb\u003E，然后首先输入一个翻译开始符 &#34;&lt;Begin&gt;&#34;，预测第一个单词 &#34;I&#34;；然后输入翻译开始符 &#34;&lt;Begin&gt;&#34; 和单词 &#34;I&#34;，预测单词 &#34;have&#34;，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。\u003C\u002Fp\u003E\u003Ch2\u003E2. Transformer 的输入\u003C\u002Fh2\u003E\u003Cp data-pid=\"P-_Ljk3V\"\u003ETransformer 中单词的输入表示 \u003Cb\u003Ex\u003C\u002Fb\u003E由\u003Cb\u003E单词 Embedding\u003C\u002Fb\u003E 和\u003Cb\u003E位置 Embedding\u003C\u002Fb\u003E （Positional Encoding）相加得到。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b0a11f97ab22f5d9ebc396bc50fa9c3f_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"142\" data-original-token=\"v2-b0a11f97ab22f5d9ebc396bc50fa9c3f\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b0a11f97ab22f5d9ebc396bc50fa9c3f_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ETransformer 的输入表示\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E2.1 单词 Embedding\u003C\u002Fh3\u003E\u003Cp data-pid=\"67OOnfVl\"\u003E单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。\u003C\u002Fp\u003E\u003Ch3\u003E2.2 位置 Embedding\u003C\u002Fh3\u003E\u003Cp data-pid=\"3qttpwzz\"\u003ETransformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。\u003Cb\u003E因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。\u003C\u002Fb\u003E所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。\u003C\u002Fp\u003E\u003Cp data-pid=\"43CgJ1x5\"\u003E位置 Embedding 用 \u003Cb\u003EPE\u003C\u002Fb\u003E表示，\u003Cb\u003EPE\u003C\u002Fb\u003E 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-8b442ffd03ea0f103e9acc37a1db910a_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"136\" data-original-token=\"v2-8b442ffd03ea0f103e9acc37a1db910a\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-8b442ffd03ea0f103e9acc37a1db910a_r.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"zgBxjiuH\"\u003E其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。使用这种公式计算 PE 有以下的好处：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"stlz419F\"\u003E使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。\u003C\u002Fli\u003E\u003Cli data-pid=\"Dj2BhT15\"\u003E可以让模型容易地计算出相对位置，对于固定长度的间距 k，\u003Cb\u003EPE(pos+k)\u003C\u002Fb\u003E 可以用 \u003Cb\u003EPE(pos)\u003C\u002Fb\u003E 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"DAcCydQ0\"\u003E将单词的词 Embedding 和位置 Embedding 相加，就可以得到单词的表示向量 \u003Cb\u003Ex\u003C\u002Fb\u003E，\u003Cb\u003Ex \u003C\u002Fb\u003E就是 Transformer 的输入。\u003C\u002Fp\u003E\u003Ch2\u003E3. Self-Attention（自注意力机制）\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f6380627207ff4d1e72addfafeaff0bb_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"884\" data-original-token=\"v2-f6380627207ff4d1e72addfafeaff0bb\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f6380627207ff4d1e72addfafeaff0bb_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ETransformer Encoder 和 Decoder\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"UusPTvfg\"\u003E上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为\u003Cb\u003E Multi-Head Attention\u003C\u002Fb\u003E，是由多个 \u003Cb\u003ESelf-Attention\u003C\u002Fb\u003E组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。\u003C\u002Fp\u003E\u003Cp data-pid=\"S_GBVCpG\"\u003E因为 \u003Cb\u003ESelf-Attention\u003C\u002Fb\u003E是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。\u003C\u002Fp\u003E\u003Ch3\u003E3.1 Self-Attention 结构\u003C\u002Fh3\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-6444601b4c41d99e70569b0ea388c3bd_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"406\" data-rawheight=\"488\" data-original-token=\"v2-6444601b4c41d99e70569b0ea388c3bd\" class=\"content_image\" width=\"406\"\u002F\u003E\u003Cfigcaption\u003ESelf-Attention 结构\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"kPoXDPJf\"\u003E上图是 Self-Attention 的结构，在计算的时候需要用到矩阵\u003Cb\u003EQ(查询),K(键值),V(值)\u003C\u002Fb\u003E。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而\u003Cb\u003EQ,K,V\u003C\u002Fb\u003E正是通过 Self-Attention 的输入进行线性变换得到的。\u003C\u002Fp\u003E\u003Ch3\u003E3.2 Q, K, V 的计算\u003C\u002Fh3\u003E\u003Cp data-pid=\"koWF61hY\"\u003ESelf-Attention 的输入用矩阵X进行表示，则可以使用线性变阵矩阵\u003Cb\u003EWQ,WK,WV\u003C\u002Fb\u003E计算得到\u003Cb\u003EQ,K,V\u003C\u002Fb\u003E。计算如下图所示，\u003Cb\u003E注意 X, Q, K, V 的每一行都表示一个单词。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-4f4958704952dcf2c4b652a1cd38f32e_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"875\" data-original-token=\"v2-4f4958704952dcf2c4b652a1cd38f32e\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-4f4958704952dcf2c4b652a1cd38f32e_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EQ, K, V 的计算\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E3.3 Self-Attention 的输出\u003C\u002Fh3\u003E\u003Cp data-pid=\"DA2kBPUp\"\u003E得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-9699a37b96c2b62d22b312b5e1863acd_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"171\" data-original-token=\"v2-9699a37b96c2b62d22b312b5e1863acd\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-9699a37b96c2b62d22b312b5e1863acd_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ESelf-Attention 的输出\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"pn9xOANa\"\u003E公式中计算矩阵\u003Cb\u003EQ\u003C\u002Fb\u003E和\u003Cb\u003EK\u003C\u002Fb\u003E每一行向量的内积，为了防止内积过大，因此除以  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d_%7Bk%7D\" alt=\"d_{k}\" eeimg=\"1\"\u002F\u003E  的平方根。\u003Cb\u003EQ\u003C\u002Fb\u003E乘以\u003Cb\u003EK\u003C\u002Fb\u003E的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为\u003Cb\u003EQ\u003C\u002Fb\u003E乘以 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K%5E%7BT%7D\" alt=\"K^{T}\" eeimg=\"1\"\u002F\u003E ，1234 表示的是句子中的单词。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-9caab2c9a00f6872854fb89278f13ee1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"228\" data-original-token=\"v2-9caab2c9a00f6872854fb89278f13ee1\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-9caab2c9a00f6872854fb89278f13ee1_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EQ乘以K的转置的计算\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"WP8aAUDY\"\u003E得到\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=QK%5E%7BT%7D\" alt=\"QK^{T}\" eeimg=\"1\"\u002F\u003E 之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1.\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-96a3716cf7f112f7beabafb59e84f418_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"247\" data-original-token=\"v2-96a3716cf7f112f7beabafb59e84f418\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-96a3716cf7f112f7beabafb59e84f418_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E对矩阵的每一行进行 Softmax\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"QgcSFJKu\"\u003E得到 Softmax 矩阵之后可以和\u003Cb\u003EV\u003C\u002Fb\u003E相乘，得到最终的输出\u003Cb\u003EZ\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7ac99bce83713d568d04e6ecfb31463b_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"217\" data-original-token=\"v2-7ac99bce83713d568d04e6ecfb31463b\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7ac99bce83713d568d04e6ecfb31463b_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ESelf-Attention 输出\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"Kn263Qnw\"\u003E上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z_%7B1%7D\" alt=\"Z_{1}\" eeimg=\"1\"\u002F\u003E  等于所有单词 i 的值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%7Bi%7D\" alt=\"V_{i}\" eeimg=\"1\"\u002F\u003E  根据 attention 系数的比例加在一起得到，如下图所示：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-27822b2292cd6c38357803093bea5d0e_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"216\" data-original-token=\"v2-27822b2292cd6c38357803093bea5d0e\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-27822b2292cd6c38357803093bea5d0e_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EZi 的计算方法\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E3.4 Multi-Head Attention\u003C\u002Fh3\u003E\u003Cp data-pid=\"S4RuF-Xc\"\u003E在上一步，我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-b0ea8f5b639786f98330f70405e94a75_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"859\" data-original-token=\"v2-b0ea8f5b639786f98330f70405e94a75\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-b0ea8f5b639786f98330f70405e94a75_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EMulti-Head Attention\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"WUq-PPHP\"\u003E从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入\u003Cb\u003EX\u003C\u002Fb\u003E分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵\u003Cb\u003EZ\u003C\u002Fb\u003E。下图是 h=8 时候的情况，此时会得到 8 个输出矩阵\u003Cb\u003EZ\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6bdaf739fd6b827b2087b4e151c560f4_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"831\" data-original-token=\"v2-6bdaf739fd6b827b2087b4e151c560f4\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6bdaf739fd6b827b2087b4e151c560f4_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E多个 Self-Attention\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"tEHtDHN1\"\u003E得到 8 个输出矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z_%7B1%7D\" alt=\"Z_{1}\" eeimg=\"1\"\u002F\u003E  到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z_%7B8%7D\" alt=\"Z_{8}\" eeimg=\"1\"\u002F\u003E 之后，Multi-Head Attention 将它们拼接在一起 \u003Cb\u003E(Concat)\u003C\u002Fb\u003E，然后传入一个\u003Cb\u003ELinear\u003C\u002Fb\u003E层，得到 Multi-Head Attention 最终的输出\u003Cb\u003EZ\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-35d78d9aa9150ae4babd0ea6aa68d113_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"388\" data-original-token=\"v2-35d78d9aa9150ae4babd0ea6aa68d113\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-35d78d9aa9150ae4babd0ea6aa68d113_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EMulti-Head Attention 的输出\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"kzWvDrWD\"\u003E可以看到 Multi-Head Attention 输出的矩阵\u003Cb\u003EZ\u003C\u002Fb\u003E与其输入的矩阵\u003Cb\u003EX\u003C\u002Fb\u003E的维度是一样的。\u003C\u002Fp\u003E\u003Ch2\u003E4. Encoder 结构\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-0203e83066913b53ec6f5482be092aa1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"884\" data-original-token=\"v2-0203e83066913b53ec6f5482be092aa1\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-0203e83066913b53ec6f5482be092aa1_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ETransformer Encoder block\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"J3mjSa2T\"\u003E上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 Multi-Head Attention,\u003Cb\u003E Add &amp; Norm, Feed Forward, Add &amp; Norm \u003C\u002Fb\u003E组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。\u003C\u002Fp\u003E\u003Ch3\u003E4.1 Add &amp; Norm\u003C\u002Fh3\u003E\u003Cp data-pid=\"1kA5s52x\"\u003EAdd &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a4b35db50f882522ee52f61ddd411a5a_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"107\" data-original-token=\"v2-a4b35db50f882522ee52f61ddd411a5a\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a4b35db50f882522ee52f61ddd411a5a_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EAdd &amp;amp;amp;amp;amp;amp; Norm 公式\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"D3Zf-0rM\"\u003E其中 \u003Cb\u003EX\u003C\u002Fb\u003E表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(\u003Cb\u003EX\u003C\u002Fb\u003E) 和 FeedForward(\u003Cb\u003EX\u003C\u002Fb\u003E) 表示输出 (输出与输入 \u003Cb\u003EX \u003C\u002Fb\u003E维度是一样的，所以可以相加)。\u003C\u002Fp\u003E\u003Cp data-pid=\"GAmriVAD\"\u003E\u003Cb\u003EAdd\u003C\u002Fb\u003E指 \u003Cb\u003EX\u003C\u002Fb\u003E+MultiHeadAttention(\u003Cb\u003EX\u003C\u002Fb\u003E)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4b3dde965124bd00f9893b05ebcaad0f_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"117\" data-original-token=\"v2-4b3dde965124bd00f9893b05ebcaad0f\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4b3dde965124bd00f9893b05ebcaad0f_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E残差连接\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"WfJc0LoJ\"\u003E\u003Cb\u003ENorm\u003C\u002Fb\u003E指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。\u003C\u002Fp\u003E\u003Ch3\u003E4.2 Feed Forward\u003C\u002Fh3\u003E\u003Cp data-pid=\"iOC9_HOt\"\u003EFeed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-47b39ca4cc3cd0be157d6803c8c8e0a1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"596\" data-rawheight=\"77\" data-original-token=\"v2-47b39ca4cc3cd0be157d6803c8c8e0a1\" class=\"origin_image zh-lightbox-thumb\" width=\"596\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-47b39ca4cc3cd0be157d6803c8c8e0a1_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EFeed Forward\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"UBzq-OVA\"\u003E\u003Cb\u003EX\u003C\u002Fb\u003E是输入，Feed Forward 最终得到的输出矩阵的维度与\u003Cb\u003EX\u003C\u002Fb\u003E一致。\u003C\u002Fp\u003E\u003Ch3\u003E4.3 组成 Encoder\u003C\u002Fh3\u003E\u003Cp data-pid=\"H1K4fEUt\"\u003E通过上面描述的 Multi-Head Attention, Feed Forward, Add &amp; Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X_%7B%28n%5Ctimes+d%29%7D\" alt=\"X_{(n\\times d)}\" eeimg=\"1\"\u002F\u003E ，并输出一个矩阵  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=O_%7B%28n%5Ctimes+d%29%7D\" alt=\"O_{(n\\times d)}\" eeimg=\"1\"\u002F\u003E 。通过多个 Encoder block 叠加就可以组成 Encoder。\u003C\u002Fp\u003E\u003Cp data-pid=\"2c4xDaYY\"\u003E第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是\u003Cb\u003E编码信息矩阵 C\u003C\u002Fb\u003E，这一矩阵后续会用到 Decoder 中。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-45db05405cb96248aff98ee07a565baa_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"900\" data-original-token=\"v2-45db05405cb96248aff98ee07a565baa\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-45db05405cb96248aff98ee07a565baa_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EEncoder 编码句子信息\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E5. Decoder 结构\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f5049e8711c3abe8f8938ced9e7fc3da_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"884\" data-original-token=\"v2-f5049e8711c3abe8f8938ced9e7fc3da\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f5049e8711c3abe8f8938ced9e7fc3da_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ETransformer Decoder block\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"-GJFcQbQ\"\u003E上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"uQ15qepT\"\u003E包含两个 Multi-Head Attention 层。\u003C\u002Fli\u003E\u003Cli data-pid=\"OYZubYSx\"\u003E第一个 Multi-Head Attention 层采用了 Masked 操作。\u003C\u002Fli\u003E\u003Cli data-pid=\"1YA1p36E\"\u003E第二个 Multi-Head Attention 层的\u003Cb\u003EK, V\u003C\u002Fb\u003E矩阵使用 Encoder 的\u003Cb\u003E编码信息矩阵C\u003C\u002Fb\u003E进行计算，而\u003Cb\u003EQ\u003C\u002Fb\u003E使用上一个 Decoder block 的输出计算。\u003C\u002Fli\u003E\u003Cli data-pid=\"rB_j5s55\"\u003E最后有一个 Softmax 层计算下一个翻译单词的概率。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003E5.1 第一个 Multi-Head Attention\u003C\u002Fh3\u003E\u003Cp data-pid=\"AXeguOLh\"\u003EDecoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 &#34;我有一只猫&#34; 翻译成 &#34;I have a cat&#34; 为例，了解一下 Masked 操作。\u003C\u002Fp\u003E\u003Cp data-pid=\"Xsgegc-q\"\u003E下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 &#34;&lt;Begin&gt;&#34; 预测出第一个单词为 &#34;I&#34;，然后根据输入 &#34;&lt;Begin&gt; I&#34; 预测下一个单词 &#34;have&#34;。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-4616451fe8aa59b2df2ead30fa31dc98_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"140\" data-original-token=\"v2-4616451fe8aa59b2df2ead30fa31dc98\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-4616451fe8aa59b2df2ead30fa31dc98_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EDecoder 预测\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"ePRyx-DQ\"\u003EDecoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 (&lt;Begin&gt; I have a cat) 和对应输出 (I have a cat &lt;end&gt;) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，\u003Cb\u003E注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 &#34;&lt;Begin&gt; I have a cat &lt;end&gt;&#34;。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"FPU6iWoo\"\u003E\u003Cb\u003E第一步：\u003C\u002Fb\u003E是 Decoder 的输入矩阵和 \u003Cb\u003EMask \u003C\u002Fb\u003E矩阵，输入矩阵包含 &#34;&lt;Begin&gt; I have a cat&#34; (0, 1, 2, 3, 4) 五个单词的表示向量，\u003Cb\u003EMask \u003C\u002Fb\u003E是一个 5×5 的矩阵。在 \u003Cb\u003EMask \u003C\u002Fb\u003E可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-b26299d383aee0dd42b163e8bda74fc8_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"277\" data-original-token=\"v2-b26299d383aee0dd42b163e8bda74fc8\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpica.zhimg.com\u002Fv2-b26299d383aee0dd42b163e8bda74fc8_r.jpg\"\u002F\u003E\u003Cfigcaption\u003E输入矩阵与 Mask 矩阵\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"g8lAyIkQ\"\u003E\u003Cb\u003E第二步：\u003C\u002Fb\u003E接下来的操作和之前的 Self-Attention 一样，通过输入矩阵\u003Cb\u003EX\u003C\u002Fb\u003E计算得到\u003Cb\u003EQ,K,V\u003C\u002Fb\u003E矩阵。然后计算\u003Cb\u003EQ\u003C\u002Fb\u003E和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K%5E%7BT%7D\" alt=\"K^{T}\" eeimg=\"1\"\u002F\u003E 的乘积 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=QK%5E%7BT%7D\" alt=\"QK^{T}\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a63ff9b965595438ed0c0e0547cd3d3b_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"240\" data-original-token=\"v2-a63ff9b965595438ed0c0e0547cd3d3b\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a63ff9b965595438ed0c0e0547cd3d3b_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EQ乘以K的转置\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"zFwaFths\"\u003E\u003Cb\u003E第三步：\u003C\u002Fb\u003E在得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=QK%5E%7BT%7D\" alt=\"QK^{T}\" eeimg=\"1\"\u002F\u003E  之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用\u003Cb\u003EMask\u003C\u002Fb\u003E矩阵遮挡住每一个单词之后的信息，遮挡操作如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-35d1c8eae955f6f4b6b3605f7ef00ee1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"206\" data-original-token=\"v2-35d1c8eae955f6f4b6b3605f7ef00ee1\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-35d1c8eae955f6f4b6b3605f7ef00ee1_r.jpg\"\u002F\u003E\u003Cfigcaption\u003ESoftmax 之前 Mask\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"LrBaRp1I\"\u003E得到 \u003Cb\u003EMask\u003C\u002Fb\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=QK%5E%7BT%7D\" alt=\"QK^{T}\" eeimg=\"1\"\u002F\u003E  之后在 \u003Cb\u003EMask \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=QK%5E%7BT%7D\" alt=\"QK^{T}\" eeimg=\"1\"\u002F\u003E上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。\u003C\u002Fp\u003E\u003Cp data-pid=\"ZKqdTv1L\"\u003E\u003Cb\u003E第四步：\u003C\u002Fb\u003E使用 \u003Cb\u003EMask \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=QK%5E%7BT%7D\" alt=\"QK^{T}\" eeimg=\"1\"\u002F\u003E与矩阵\u003Cb\u003E V\u003C\u002Fb\u003E相乘，得到输出 \u003Cb\u003EZ\u003C\u002Fb\u003E，则单词 1 的输出向量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z_%7B1%7D\" alt=\"Z_{1}\" eeimg=\"1\"\u002F\u003E 是只包含单词 1 信息的。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-58f916c806a6981e296a7a699151af87_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"292\" data-original-token=\"v2-58f916c806a6981e296a7a699151af87\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-58f916c806a6981e296a7a699151af87_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EMask 之后的输出\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"4_h9O2No\"\u003E\u003Cb\u003E第五步：\u003C\u002Fb\u003E通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z_%7Bi%7D\" alt=\"Z_{i}\" eeimg=\"1\"\u002F\u003E ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z_%7Bi%7D\" alt=\"Z_{i}\" eeimg=\"1\"\u002F\u003E 然后计算得到第一个 Multi-Head Attention 的输出\u003Cb\u003EZ\u003C\u002Fb\u003E，\u003Cb\u003EZ\u003C\u002Fb\u003E与输入\u003Cb\u003EX\u003C\u002Fb\u003E维度一样。\u003C\u002Fp\u003E\u003Ch3\u003E5.2 第二个 Multi-Head Attention\u003C\u002Fh3\u003E\u003Cp data-pid=\"BFqZNiKC\"\u003EDecoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 \u003Cb\u003EK, V\u003C\u002Fb\u003E矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 \u003Cb\u003EEncoder 的编码信息矩阵 C \u003C\u002Fb\u003E计算的。\u003C\u002Fp\u003E\u003Cp data-pid=\"DPtM9EeZ\"\u003E根据 Encoder 的输出 \u003Cb\u003EC\u003C\u002Fb\u003E计算得到 \u003Cb\u003EK, V\u003C\u002Fb\u003E，根据上一个 Decoder block 的输出\u003Cb\u003E Z\u003C\u002Fb\u003E 计算 \u003Cb\u003EQ\u003C\u002Fb\u003E (如果是第一个 Decoder block 则使用输入矩阵 \u003Cb\u003EX\u003C\u002Fb\u003E 进行计算)，后续的计算方法与之前描述的一致。\u003C\u002Fp\u003E\u003Cp data-pid=\"AcK1hMgP\"\u003E这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 \u003Cb\u003EMask\u003C\u002Fb\u003E)。\u003C\u002Fp\u003E\u003Ch3\u003E5.3 Softmax 预测输出单词\u003C\u002Fh3\u003E\u003Cp data-pid=\"ScHGVY2r\"\u003EDecoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-335cfa1b345bdd5cf1e212903bb9b185_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"239\" data-original-token=\"v2-335cfa1b345bdd5cf1e212903bb9b185\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-335cfa1b345bdd5cf1e212903bb9b185_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EDecoder Softmax 之前的 Z\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"5QPkXQBP\"\u003ESoftmax 根据输出矩阵的每一行预测下一个单词：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0938aa45a288b5d6bef6487efe53bd9d_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"357\" data-original-token=\"v2-0938aa45a288b5d6bef6487efe53bd9d\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0938aa45a288b5d6bef6487efe53bd9d_r.jpg\"\u002F\u003E\u003Cfigcaption\u003EDecoder Softmax 预测\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"6jjQp-dz\"\u003E这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。\u003C\u002Fp\u003E\u003Ch2\u003E6. Transformer 总结\u003C\u002Fh2\u003E\u003Cul\u003E\u003Cli data-pid=\"hI3YKOP1\"\u003ETransformer 与 RNN 不同，可以比较好地并行训练。\u003C\u002Fli\u003E\u003Cli data-pid=\"3PSOM3Hw\"\u003ETransformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。\u003C\u002Fli\u003E\u003Cli data-pid=\"5fAv8TFH\"\u003ETransformer 的重点是 Self-Attention 结构，其中用到的 \u003Cb\u003EQ, K, V\u003C\u002Fb\u003E矩阵通过输出进行线性变换得到。\u003C\u002Fli\u003E\u003Cli data-pid=\"B77KoJQW\"\u003ETransformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"tFcdbALf\"\u003E\u003Csup data-text=\"论文:Attention Is All You Need\" data-url=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1706.03762\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"1\"\u003E[1]\u003C\u002Fsup\u003E\u003Csup data-text=\"Transformer 模型详解\" data-url=\"https:\u002F\u002Fbaijiahao.baidu.com\u002Fs?id=1651219987457222196&amp;wfr=spider&amp;for=pc\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"2\"\u003E[2]\u003C\u002Fsup\u003E\u003C\u002Fp\u003E","contentNeedTruncated":false,"adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20746363","type":"topic","id":"20746363","name":"Transformer"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19560026","type":"topic","id":"19560026","name":"自然语言处理"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19813032","type":"topic","id":"19813032","name":"深度学习（Deep Learning）"}],"voteupCount":12909,"voting":0,"heavyUpStatus":"allow_heavy_up","column":{"description":"计算机视觉是深度学习领域最热门的研究领域之一。计算机视觉实际上是一个跨领域的交叉学科，包括计算机科学（图形、算法、理论、系统、体系结构），数学（信息检索、机器学习），工程学（机器人、语音、自然语言处理、图像处理），物理学（光学 ），生物学（神经科学）和心理学（认知科学）等等。许多科学家认为，计算机视觉为人工智能的发展开拓了道路。","canManage":false,"intro":"从这里开始认识人类的眼睛——计算机视觉","isFollowing":false,"urlToken":"c_1186688096946528256","id":"c_1186688096946528256","articlesCount":74,"acceptSubmission":true,"title":"初识CV","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1186688096946528256","commentPermission":"all","created":1575708489,"updated":1591455834,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-16340cfaf16380019c183d160df3bb5e.jpg?source=172ae18b","uid":"754475696817983488","userType":"people","isFollowing":false,"urlToken":"AI_team-WSF","id":"7d0bf3ef6a7f044754895a3752969515","description":"个人研究方向：1. 图像（语音，视频）分类、图像（视频）检测、图像（视频）分割、图像（视频）降噪、图像（视频）超分；2. 模型压缩（模型量化，模型剪枝，模型蒸馏，模型稀疏）；3. 脉冲神经网络。欢迎大家一起交流学习～","name":"初识CV","isAdvertiser":false,"headline":"美好年华，扬起理想之帆，踏上新的征程加油！","gender":1,"url":"\u002Fpeople\u002F7d0bf3ef6a7f044754895a3752969515","avatarUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-16340cfaf16380019c183d160df3bb5e_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}},"followers":550,"type":"column","columnType":"normal"},"commentCount":545,"contributions":[{"id":28075797,"state":"accepted","type":"first_publish","column":{"description":"计算机视觉是深度学习领域最热门的研究领域之一。计算机视觉实际上是一个跨领域的交叉学科，包括计算机科学（图形、算法、理论、系统、体系结构），数学（信息检索、机器学习），工程学（机器人、语音、自然语言处理、图像处理），物理学（光学 ），生物学（神经科学）和心理学（认知科学）等等。许多科学家认为，计算机视觉为人工智能的发展开拓了道路。","canManage":false,"intro":"从这里开始认识人类的眼睛——计算机视觉","isFollowing":false,"urlToken":"c_1186688096946528256","id":"c_1186688096946528256","articlesCount":74,"acceptSubmission":true,"title":"初识CV","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1186688096946528256","commentPermission":"all","created":1575708489,"updated":1591455834,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-16340cfaf16380019c183d160df3bb5e.jpg?source=172ae18b","uid":"754475696817983488","userType":"people","isFollowing":false,"urlToken":"AI_team-WSF","id":"7d0bf3ef6a7f044754895a3752969515","description":"个人研究方向：1. 图像（语音，视频）分类、图像（视频）检测、图像（视频）分割、图像（视频）降噪、图像（视频）超分；2. 模型压缩（模型量化，模型剪枝，模型蒸馏，模型稀疏）；3. 脉冲神经网络。欢迎大家一起交流学习～","name":"初识CV","isAdvertiser":false,"headline":"美好年华，扬起理想之帆，踏上新的征程加油！","gender":1,"url":"\u002Fpeople\u002F7d0bf3ef6a7f044754895a3752969515","avatarUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-16340cfaf16380019c183d160df3bb5e_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}},"followers":550,"type":"column","columnType":"normal"}},{"id":30162100,"state":"accepted","type":"first_publish","column":{"description":"介绍一些自己工作、竞赛中的经验","canManage":false,"intro":"数据竞赛经验，工作积累的笔记，南湖边有可爱的橘猫","isFollowing":false,"urlToken":"c_1173652984163610624","id":"c_1173652984163610624","articlesCount":105,"acceptSubmission":true,"title":"南湖研究院","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1173652984163610624","commentPermission":"all","created":1572600676,"updated":1606269840,"imageUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-674781ef13a310d6045598d915896623_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-38f64d2f7495091543123b79f8e4095d.jpg?source=172ae18b","uid":"611596736741904384","userType":"people","isFollowing":false,"urlToken":"yi-xian-wei-50","id":"7b36531fa2a2739b1e85f1c85f9022fc","description":"系统之神与我同在队队长 工业智能硬件 机器人研发管理\n2018 平安产险数据建模大赛 驾驶行为预测驾驶风险，Top3\n2019 CCF BDCI 基于OCR的身份证要素提取，Top4\n2020 科技战疫 疫情期间网民情绪识别，Top2\n2020 CCF BDCI 人工智能解小学应用题，Top5\n2021 CCKS 图谱问答，Top3\n2021 国家电网智能信息检索 NL2SQL Top3\n2022 CCKS 图谱问答 Top4\n2022 CCKS NL2SQL Top2\n2022 百度飞桨平台 蜜度文本智能校对大赛 Top3\n2022 AntSQL大规模金融语义解析中文Text-to-SQL挑战赛 Top2\n2022 CHIP 生成决策诊疗树 Top2","name":"易显维","isAdvertiser":false,"headline":"系统之神与我同在，竞赛top选手，全栈算法研发","gender":1,"url":"\u002Fpeople\u002F7b36531fa2a2739b1e85f1c85f9022fc","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-38f64d2f7495091543123b79f8e4095d_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}},"followers":179,"type":"column","columnType":"normal"}},{"id":33166040,"state":"accepted","type":"first_publish","column":{"description":"","canManage":false,"intro":"记录不断发展的计算机知识。","isFollowing":false,"urlToken":"c_1339338855846608896","id":"c_1339338855846608896","articlesCount":127,"acceptSubmission":false,"title":"深度视觉与自然语言探究","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1339338855846608896","commentPermission":"all","created":1612103267,"updated":1612103267,"imageUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5ab8f7a71826c2ad330d8dd3c27e7bee.jpg?source=172ae18b","uid":"906986482877730816","userType":"people","isFollowing":false,"urlToken":"liousiaocing","id":"e50a69535479b2cc44ec4e329a464252","description":"","name":"Liou Siaocing","isAdvertiser":false,"headline":"不断学习是活下去的基础","gender":-1,"url":"\u002Fpeople\u002Fe50a69535479b2cc44ec4e329a464252","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5ab8f7a71826c2ad330d8dd3c27e7bee_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}},"followers":19,"type":"column","columnType":"normal"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":true,"tipjarorsCount":12,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"favlistsCount":27276,"isNormal":true,"status":0,"activityToppingInfo":{"state":"untopped"},"shareText":"Transformer模型详解（图解最完整版） - 来自知乎专栏「初识CV」，作者: 初识CV https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F338817680 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":3359,"hasColumn":true,"republishers":[{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5ab8f7a71826c2ad330d8dd3c27e7bee.jpg?source=172ae18b","uid":"906986482877730816","userType":"people","isFollowing":false,"urlToken":"liousiaocing","id":"e50a69535479b2cc44ec4e329a464252","description":"","name":"Liou Siaocing","isAdvertiser":false,"headline":"不断学习是活下去的基础","gender":-1,"url":"\u002Fpeople\u002Fe50a69535479b2cc44ec4e329a464252","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5ab8f7a71826c2ad330d8dd3c27e7bee_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}},{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-38f64d2f7495091543123b79f8e4095d.jpg?source=172ae18b","uid":"611596736741904384","userType":"people","isFollowing":false,"urlToken":"yi-xian-wei-50","id":"7b36531fa2a2739b1e85f1c85f9022fc","description":"系统之神与我同在队队长 工业智能硬件 机器人研发管理\n2018 平安产险数据建模大赛 驾驶行为预测驾驶风险，Top3\n2019 CCF BDCI 基于OCR的身份证要素提取，Top4\n2020 科技战疫 疫情期间网民情绪识别，Top2\n2020 CCF BDCI 人工智能解小学应用题，Top5\n2021 CCKS 图谱问答，Top3\n2021 国家电网智能信息检索 NL2SQL Top3\n2022 CCKS 图谱问答 Top4\n2022 CCKS NL2SQL Top2\n2022 百度飞桨平台 蜜度文本智能校对大赛 Top3\n2022 AntSQL大规模金融语义解析中文Text-to-SQL挑战赛 Top2\n2022 CHIP 生成决策诊疗树 Top2","name":"易显维","isAdvertiser":false,"headline":"系统之神与我同在，竞赛top选手，全栈算法研发","gender":1,"url":"\u002Fpeople\u002F7b36531fa2a2739b1e85f1c85f9022fc","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-38f64d2f7495091543123b79f8e4095d_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}}],"isNewLinkCard":true,"emojiReaction":{"likeCount":3359,"likeHasSet":false},"abParam":{"qaFotoolbar":"0","qaHiddenVoteup":"","qaNewdownvote":"1","rsInterest1":"","zpZhiStyle":""},"attachedInfo":"kgIkCgkxNjM0MjI5NzkSCTMzODgxNzY4MBgHIgpJTUFHRV9URVhU","shareGuide":{"hasPositiveBubble":true,"hasTimeBubble":true,"hitShareGuideCluster":true},"settings":{"tableOfContents":{"enabled":true}},"canReference":false,"reactionInstruction":{"rEACTIONCONTENTSEGMENTLIKE":"HIDE"},"reaction":{"statistics":{"upVoteCount":12909,"downVoteCount":204,"likeCount":3359,"commentCount":545,"shareCount":0,"playCount":0,"interestPlayCount":0,"favorites":27276,"pvCount":0,"bulletCount":0,"applaudCount":0,"questionFollowerCount":0,"questionAnswerCount":0,"plaincontentVoteUpCount":0,"plaincontentLikeCount":0,"imgLikeCount":{"v20203e83066913b53ec6f5482be092aa1":6,"v20938aa45a288b5d6bef6487efe53bd9d":2,"v227822b2292cd6c38357803093bea5d0e":8,"v2335cfa1b345bdd5cf1e212903bb9b185":1,"v235d1c8eae955f6f4b6b3605f7ef00ee1":1,"v235d78d9aa9150ae4babd0ea6aa68d113":7,"v24544255f3f24b7af1e520684ae38403f":12,"v245db05405cb96248aff98ee07a565baa":14,"v24616451fe8aa59b2df2ead30fa31dc98":0,"v247b39ca4cc3cd0be157d6803c8c8e0a1":0,"v24b3dde965124bd00f9893b05ebcaad0f":0,"v24f4958704952dcf2c4b652a1cd38f32e":16,"v25367bd47a2319397317562c0da77e455":22,"v258f916c806a6981e296a7a699151af87":1,"v26444601b4c41d99e70569b0ea388c3bd":15,"v26bdaf739fd6b827b2087b4e151c560f4":5,"v27ac99bce83713d568d04e6ecfb31463b":3,"v27be8fe269991a236f000168291481c8b":0,"v27dd39c44b0ae45d31a3ae7f39d3f883f":28,"v28b442ffd03ea0f103e9acc37a1db910a":0,"v29699a37b96c2b62d22b312b5e1863acd":4,"v296a3716cf7f112f7beabafb59e84f418":11,"v29caab2c9a00f6872854fb89278f13ee1":11,"v2A4b35db50f882522ee52f61ddd411a5a":0,"v2A63ff9b965595438ed0c0e0547cd3d3b":4,"v2B0a11f97ab22f5d9ebc396bc50fa9c3f":0,"v2B0ea8f5b639786f98330f70405e94a75":4,"v2B26299d383aee0dd42b163e8bda74fc8":7,"v2F5049e8711c3abe8f8938ced9e7fc3da":9,"v2F6380627207ff4d1e72addfafeaff0bb":18}},"relation":{"isAuthor":false,"vote":"Neutral","liked":false,"imgLiked":{"v20203e83066913b53ec6f5482be092aa1":false,"v20938aa45a288b5d6bef6487efe53bd9d":false,"v227822b2292cd6c38357803093bea5d0e":false,"v2335cfa1b345bdd5cf1e212903bb9b185":false,"v235d1c8eae955f6f4b6b3605f7ef00ee1":false,"v235d78d9aa9150ae4babd0ea6aa68d113":false,"v24544255f3f24b7af1e520684ae38403f":false,"v245db05405cb96248aff98ee07a565baa":false,"v24616451fe8aa59b2df2ead30fa31dc98":false,"v247b39ca4cc3cd0be157d6803c8c8e0a1":false,"v24b3dde965124bd00f9893b05ebcaad0f":false,"v24f4958704952dcf2c4b652a1cd38f32e":false,"v25367bd47a2319397317562c0da77e455":false,"v258f916c806a6981e296a7a699151af87":false,"v26444601b4c41d99e70569b0ea388c3bd":false,"v26bdaf739fd6b827b2087b4e151c560f4":false,"v27ac99bce83713d568d04e6ecfb31463b":false,"v27be8fe269991a236f000168291481c8b":false,"v27dd39c44b0ae45d31a3ae7f39d3f883f":false,"v28b442ffd03ea0f103e9acc37a1db910a":false,"v29699a37b96c2b62d22b312b5e1863acd":false,"v296a3716cf7f112f7beabafb59e84f418":false,"v29caab2c9a00f6872854fb89278f13ee1":false,"v2A4b35db50f882522ee52f61ddd411a5a":false,"v2A63ff9b965595438ed0c0e0547cd3d3b":false,"v2B0a11f97ab22f5d9ebc396bc50fa9c3f":false,"v2B0ea8f5b639786f98330f70405e94a75":false,"v2B26299d383aee0dd42b163e8bda74fc8":false,"v2F5049e8711c3abe8f8938ced9e7fc3da":false,"v2F6380627207ff4d1e72addfafeaff0bb":false},"faved":false,"following":false},"imageReactions":{"v20203e83066913b53ec6f5482be092aa1":{"likeCount":6,"isLiked":false},"v20938aa45a288b5d6bef6487efe53bd9d":{"likeCount":2,"isLiked":false},"v227822b2292cd6c38357803093bea5d0e":{"likeCount":8,"isLiked":false},"v2335cfa1b345bdd5cf1e212903bb9b185":{"likeCount":1,"isLiked":false},"v235d1c8eae955f6f4b6b3605f7ef00ee1":{"likeCount":1,"isLiked":false},"v235d78d9aa9150ae4babd0ea6aa68d113":{"likeCount":7,"isLiked":false},"v24544255f3f24b7af1e520684ae38403f":{"likeCount":12,"isLiked":false},"v245db05405cb96248aff98ee07a565baa":{"likeCount":14,"isLiked":false},"v24616451fe8aa59b2df2ead30fa31dc98":{"likeCount":0,"isLiked":false},"v247b39ca4cc3cd0be157d6803c8c8e0a1":{"likeCount":0,"isLiked":false},"v24b3dde965124bd00f9893b05ebcaad0f":{"likeCount":0,"isLiked":false},"v24f4958704952dcf2c4b652a1cd38f32e":{"likeCount":16,"isLiked":false},"v25367bd47a2319397317562c0da77e455":{"likeCount":22,"isLiked":false},"v258f916c806a6981e296a7a699151af87":{"likeCount":1,"isLiked":false},"v26444601b4c41d99e70569b0ea388c3bd":{"likeCount":15,"isLiked":false},"v26bdaf739fd6b827b2087b4e151c560f4":{"likeCount":5,"isLiked":false},"v27ac99bce83713d568d04e6ecfb31463b":{"likeCount":3,"isLiked":false},"v27be8fe269991a236f000168291481c8b":{"likeCount":0,"isLiked":false},"v27dd39c44b0ae45d31a3ae7f39d3f883f":{"likeCount":28,"isLiked":false},"v28b442ffd03ea0f103e9acc37a1db910a":{"likeCount":0,"isLiked":false},"v29699a37b96c2b62d22b312b5e1863acd":{"likeCount":4,"isLiked":false},"v296a3716cf7f112f7beabafb59e84f418":{"likeCount":11,"isLiked":false},"v29caab2c9a00f6872854fb89278f13ee1":{"likeCount":11,"isLiked":false},"v2A4b35db50f882522ee52f61ddd411a5a":{"likeCount":0,"isLiked":false},"v2A63ff9b965595438ed0c0e0547cd3d3b":{"likeCount":4,"isLiked":false},"v2B0a11f97ab22f5d9ebc396bc50fa9c3f":{"likeCount":0,"isLiked":false},"v2B0ea8f5b639786f98330f70405e94a75":{"likeCount":4,"isLiked":false},"v2B26299d383aee0dd42b163e8bda74fc8":{"likeCount":7,"isLiked":false},"v2F5049e8711c3abe8f8938ced9e7fc3da":{"likeCount":9,"isLiked":false},"v2F6380627207ff4d1e72addfafeaff0bb":{"likeCount":18,"isLiked":false}}},"interactionBarPlugins":[{"type":"comment","comment":{"enable":true,"placeholder":"发条带图评论"}}],"barPluginsFlipTime":3000,"podcastAudioEnter":{"text":"101 人听过","textColor":"MapBrand","textSize":13,"actionUrl":"zhihu:\u002F\u002Fpodcast\u002Faudio_player\u002F0?contentId=338817680&contentType=article"}}},"columns":{"c_1186688096946528256":{"description":"计算机视觉是深度学习领域最热门的研究领域之一。计算机视觉实际上是一个跨领域的交叉学科，包括计算机科学（图形、算法、理论、系统、体系结构），数学（信息检索、机器学习），工程学（机器人、语音、自然语言处理、图像处理），物理学（光学 ），生物学（神经科学）和心理学（认知科学）等等。许多科学家认为，计算机视觉为人工智能的发展开拓了道路。","canManage":false,"intro":"从这里开始认识人类的眼睛——计算机视觉","isFollowing":false,"urlToken":"c_1186688096946528256","id":"c_1186688096946528256","articlesCount":74,"acceptSubmission":true,"title":"初识CV","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1186688096946528256","commentPermission":"all","created":1575708489,"updated":1591455834,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-16340cfaf16380019c183d160df3bb5e.jpg?source=172ae18b","uid":"754475696817983488","userType":"people","isFollowing":false,"urlToken":"AI_team-WSF","id":"7d0bf3ef6a7f044754895a3752969515","description":"个人研究方向：1. 图像（语音，视频）分类、图像（视频）检测、图像（视频）分割、图像（视频）降噪、图像（视频）超分；2. 模型压缩（模型量化，模型剪枝，模型蒸馏，模型稀疏）；3. 脉冲神经网络。欢迎大家一起交流学习～","name":"初识CV","isAdvertiser":false,"headline":"美好年华，扬起理想之帆，踏上新的征程加油！","gender":1,"url":"\u002Fpeople\u002F7d0bf3ef6a7f044754895a3752969515","avatarUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-16340cfaf16380019c183d160df3bb5e_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}},"followers":550,"type":"column","columnType":"normal"},"c_1173652984163610624":{"description":"介绍一些自己工作、竞赛中的经验","canManage":false,"intro":"数据竞赛经验，工作积累的笔记，南湖边有可爱的橘猫","isFollowing":false,"urlToken":"c_1173652984163610624","id":"c_1173652984163610624","articlesCount":105,"acceptSubmission":true,"title":"南湖研究院","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1173652984163610624","commentPermission":"all","created":1572600676,"updated":1606269840,"imageUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-674781ef13a310d6045598d915896623_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-38f64d2f7495091543123b79f8e4095d.jpg?source=172ae18b","uid":"611596736741904384","userType":"people","isFollowing":false,"urlToken":"yi-xian-wei-50","id":"7b36531fa2a2739b1e85f1c85f9022fc","description":"系统之神与我同在队队长 工业智能硬件 机器人研发管理\n2018 平安产险数据建模大赛 驾驶行为预测驾驶风险，Top3\n2019 CCF BDCI 基于OCR的身份证要素提取，Top4\n2020 科技战疫 疫情期间网民情绪识别，Top2\n2020 CCF BDCI 人工智能解小学应用题，Top5\n2021 CCKS 图谱问答，Top3\n2021 国家电网智能信息检索 NL2SQL Top3\n2022 CCKS 图谱问答 Top4\n2022 CCKS NL2SQL Top2\n2022 百度飞桨平台 蜜度文本智能校对大赛 Top3\n2022 AntSQL大规模金融语义解析中文Text-to-SQL挑战赛 Top2\n2022 CHIP 生成决策诊疗树 Top2","name":"易显维","isAdvertiser":false,"headline":"系统之神与我同在，竞赛top选手，全栈算法研发","gender":1,"url":"\u002Fpeople\u002F7b36531fa2a2739b1e85f1c85f9022fc","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-38f64d2f7495091543123b79f8e4095d_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}},"followers":179,"type":"column","columnType":"normal"},"c_1339338855846608896":{"description":"","canManage":false,"intro":"记录不断发展的计算机知识。","isFollowing":false,"urlToken":"c_1339338855846608896","id":"c_1339338855846608896","articlesCount":127,"acceptSubmission":false,"title":"深度视觉与自然语言探究","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1339338855846608896","commentPermission":"all","created":1612103267,"updated":1612103267,"imageUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5ab8f7a71826c2ad330d8dd3c27e7bee.jpg?source=172ae18b","uid":"906986482877730816","userType":"people","isFollowing":false,"urlToken":"liousiaocing","id":"e50a69535479b2cc44ec4e329a464252","description":"","name":"Liou Siaocing","isAdvertiser":false,"headline":"不断学习是活下去的基础","gender":-1,"url":"\u002Fpeople\u002Fe50a69535479b2cc44ec4e329a464252","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5ab8f7a71826c2ad330d8dd3c27e7bee_l.jpg?source=172ae18b","isOrg":false,"type":"people","badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""}},"followers":19,"type":"column","columnType":"normal"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"zvideos":{},"eduCourses":{}},"currentUser":"2da331a7fd49661a3a43dbbe4a1218d4","account":{"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false},"cardUserInfo":{"vipInfo":{}},"handleWidget":{},"widgetList":[],"userWidgetId":""},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{},"infinity":{},"batchUsers":{},"profileInfinity":null},"env":{"abV2":{"config":{"paramMap":{"pc_mou_cre_remove":{"value":"1","abId":"rl-mou_create_remove-1"},"nm_public_library":{"value":"0"},"ws_new_call":{"value":"1","abId":"rl-mobileweb_call-1"},"ws_must_login":{"value":"1","abId":"rl-must_login-1"},"ws_hot_activate":{"value":"1","abId":"hot_activate-1"},"ws_zhida_pro":{"value":"1","abId":"rl-zhida_pro-1"},"ws_qiangzhisafe":{"value":"1","abId":"rl-qiangzhisafe_copy-1"}},"abMap":{"rl-mou_create_remove-1":{"abId":"rl-mou_create_remove-1","layerId":"rl-mou_create_remove","diversionType":2},"rl-mobileweb_call-1":{"abId":"rl-mobileweb_call-1","layerId":"rl-mobileweb_call"},"rl-must_login-1":{"abId":"rl-must_login-1","layerId":"rl-must_login"},"hot_activate-1":{"abId":"hot_activate-1","layerId":"web_standard_domain_layer8"},"rl-zhida_pro-1":{"abId":"rl-zhida_pro-1","layerId":"rl-zhida_pro","diversionType":2},"rl-qiangzhisafe_copy-1":{"abId":"rl-qiangzhisafe_copy-1","layerId":"rl-qiangzhisafe_copy"}}},"triggers":{}},"userAgent":{"Edge":false,"IE":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"HarmonyOS":false,"iOS":false,"isAppleDevice":true,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Quark":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"WxMiniProgram":false,"BaiduMiniProgram":false,"QQMiniProgram":false,"JDMiniProgram":false,"isWebView":false,"isMiniProgram":false,"origin":"Mozilla\u002F5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F131.0.0.0 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F338817680","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F338817680","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"beijing":true,"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false,"oppoSearch":false,"baiduSearch":false,"googleSearch":false,"shenma":false,"miniProgram":false,"xiaomi":false,"huaweiSearch":false},"theme":"light","appHeaderTheme":{"current":"normal","disable":true,"normal":{"bgColor":"GBK99A"},"custom":{"bgColor":"GBK99A"}},"enableShortcut":true,"referer":"","xUDId":"AEDSWOmDYBmPTl_JF1dtUYO4tg2R9psP8rQ=","mode":"ssr","conf":{},"xTrafficFreeOrigin":"","ipInfo":{"cityName":"北京","countryName":"中国","regionName":"北京","countryCode":"CN"},"logged":true,"query":{},"vars":{"passThroughHeaders":{}}},"me":{"columnContributions":[]},"label":{},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0}},"recommend":{"recommendTimes":{}}},"explore":{},"levelUpperLimit":10,"mcn":{},"mcnManage":{},"tasks":{},"announcement":{},"creatorsRecommendInfo":{}},"creators":{"common":{"applyStatus":{},"rightsStatus":{}},"bayesDomains":{"status":{},"options":{"topDomains":null,"allDomains":null,"editable":0},"contents":null},"school":{"tabs":[],"contents":[],"banner":null,"entities":{}},"faq":{"tabs":[],"article":{}},"knowledgeIncome":{},"safeguardRights":{},"analytics":{"all":{},"answer":{},"zvideo":{},"article":{},"pin":{},"singleContent":{}},"account":{"growthLevel":{}},"KMResource":{},"training":{},"ToolsQuestion":{"goodatTopics":[]},"ToolsHotspot":{"domains":[]},"ToolsRecommend":{},"ToolsCustomPromotion":{"itemLists":{},"baseInfo":{}},"ToolsSearchQuestion":{},"editorSetting":{},"MCNManage":{},"knowledgeTasks":{},"incomeAnalysis":{"income":{"aggregation":{}}},"creationManage":{"editModal":{"status":false}},"activity":{},"announcement":{},"home":{"currentCreatorUrlToken":null,"rights":[],"newRights":[],"scoreInfo":{},"menusShowControlByServer":{"bVipRecomend":false,"creationRelationship":false},"newTasks":{"creatorTask":{"tasks":[],"des":[]}},"bannerList":[],"recentlyCreated":[],"homecard":{},"homeData":{}},"videoSupport":{"textBenefit":{}},"videoDistribution":{},"profilePoster":{"creatorPosterConfig":{},"creatorPosterData":{}}},"answers":{"voters":{},"upvoters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"entityWords":{},"paidContent":{},"settings":{},"relationEndorsement":{},"growthCardOrder":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{},"upvoters":{},"relationEndorsement":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotList":[],"hotListHeadZone":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]},"hotDaily":{"data":[],"paging":{}},"hotHighlight":{"isFetching":false,"isDrained":false,"data":[],"paging":{}},"banner":{},"commercialBanner":{"show":false,"banner":{},"trackData":{}},"video":{"items":[],"next":null,"isLoading":false,"isDrained":false}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"c_1186688096946528256","c_1173652984163610624","c_1339338855846608896"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[]},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":null,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"userProfit":{"permission":{"permissionStatus":{"zhiZixuan":0,"recommend":-1,"task":0,"plugin":0,"infinity":0},"visible":false},"linkCardLimit":0},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[],"lists":{},"banners":{},"protocolStatus":{"isAgreedNew":true,"isAgreedOld":true},"probationCountdownDays":0},"zvideos":{"campaignVideoList":{},"campaigns":{},"tagoreCategory":[],"recommendations":{},"insertable":{},"recruit":{"form":{"platform":"","nickname":"","followerCount":"","domain":"","contact":""},"submited":false,"ranking":[]},"qyActivityData":{},"talkActivityData":{},"party2022ActivityData":{},"batchVideos":{},"creationReferences":{},"zvideoCollection":{},"zvideoGrant":{},"collectData":{"isFetching":false,"list":[]},"videoSource":{"isLoaded":false},"paidColumnInfo":{}},"republish":{},"commentPermission":{},"creatorRightStatus":{"list":[]},"adPromotion":{"answer":{},"article":{}},"paidColumn":{"entities":{}},"followingColumns":{},"hotSpot":{"article":{"338817680":[{"title":"任天堂 Switch 2 首支预告片公开，视频中有哪些细节值得关注？","heatScore":487,"picUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-ae973c8bdf06bf153b56de1a78bfceed.jpg?source=70618c14","ctype":"QUESTION","ctoken":"9764856171","rank":0},{"title":"上海成为中国第一个 5 万亿元GDP城市，这个数字是什么概念？","heatScore":390,"picUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c4ea2cf2874e37de62190e781529b3dd.jpg?source=70618c14","ctype":"QUESTION","ctoken":"9722834853","rank":1},{"title":"蚂蚁集团确认支付宝出现重大事故，所有订单优惠 20%，到底哪里出错了？谁将为这次错误买单？","heatScore":246,"picUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9f52b4b15f1e80971bbdb900664151db.jpg?source=70618c14","ctype":"QUESTION","ctoken":"9752934214","rank":2},{"title":"传万科总裁祝九胜被公安机关带走，政府专班已介入，万科或面临接管改组，具体情况如何？对地产行业有何影响？","heatScore":106,"picUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-efe1e96359364918455bf9ef87997781.jpg?source=70618c14","ctype":"QUESTION","ctoken":"9765417303","rank":3}]}}},"fetchHost":"www.zhihu.com","subAppName":"column","spanName":"Post","canaryConfig":{"test_canary":"0","use_new_player":"0","player_vendor":"0","use_hevc":"1","upload_use_signature":"1","use_backdrop_blur":"1","article_title_imagex":"1","play_station":"1","use_cached_supported_countries":"1"}}</script><script crossorigin="" src="https://static.zhihu.com/heifetz/vendor.4d4559ddd761145da535.js"></script><script crossorigin="" src="https://static.zhihu.com/event/react@17.0.2/umd/react.production.min.js"></script><script crossorigin="" src="https://static.zhihu.com/event/react-dom@17.0.2/umd/react-dom.production.min.js"></script><script crossorigin="" src="https://static.zhihu.com/event/react-dom@17.0.2/umd/react-dom-server.browser.production.min.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/runtime.app.3936041c03d08d216de0.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-75fc9c18.app.3db651c252e14ef6658e.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-29107295.app.42d07f814b7b05187671.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-79b5cf47.app.8b6b6bf4b6d894db9b07.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-330004dc.app.7437cf54ac28fca0e302.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-f3572862.app.88776a5ab04556b70cc2.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-0e5ce61e.app.22c35ce7e8956cfacf07.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-83b0f42f.app.4af446e71092c79a4337.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-38cf5c11.app.c0ead2ce5c3ab6da63bd.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-2ec050f6.app.afae273b9b563742d9fe.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/lib-9b20c40c.app.ce8748f4bea96d8ae847.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/2779.app.4e6c48da72f38dd6817e.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/column.app.a7cfec37eb18d13d4945.js"></script><script defer="" src="https://static.zhihu.com/event/wza/4633_1/aria.js?appid=a3637ace5dc3a347f6863b0bac487599" id="ariascripts" wapForceOldFixed="false" loadData="false"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script></html>